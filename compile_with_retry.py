#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import argparse
import os
import re
import subprocess
import sys
import time
import shutil
from pathlib import Path
import glob
import hashlib

try:
    import requests
    from bs4 import BeautifulSoup
    LIBS_AVAILABLE = True
except ImportError:
    LIBS_AVAILABLE = False
    print("è­¦å‘Š: æœªå®‰è£… requests å’Œ beautifulsoup4ï¼Œlua-neturl ä¸‹è½½ä¿®å¤ä¸å¯ç”¨")

# OOM é«˜é£é™©åŒ…åˆ—è¡¨
OOM_PRONE_PACKAGE_PATTERNS = [
    r'/gcc-\d+', r'/llvm-\d+', r'/qt5base-\d+', r'/webkitgtk-\d+', r'/linux-\d+'
]

# --- Global variable to store log content for fix functions ---
# While not ideal OOP, it simplifies passing log data to many fix functions.
log_content_global = ""
# --- Global flag for pre-computation steps ---
needs_base_files_precompute = False


def get_relative_path(path):
    """è·å–ç›¸å¯¹è·¯å¾„ï¼Œä¼˜å…ˆç›¸å¯¹äºå½“å‰å·¥ä½œç›®å½•"""
    current_pwd = os.getcwd()
    try:
        # Ensure path is absolute first
        abs_path = Path(path).resolve()
        # Check if it's within the current working directory
        if abs_path.is_relative_to(current_pwd):
            return str(abs_path.relative_to(current_pwd))
        else:
            # Return absolute path if outside CWD
            return str(abs_path)
    except (ValueError, OSError, Exception): # Handle various errors like non-existence or cross-drive issues
        # Fallback to the original path string if resolution/relpath fails
        return str(path)

# --- Error Signature Detection ---
def get_error_signature(log_content):
    """Detects a specific error signature from the build log."""
    if not log_content: return "no_log_content"

    # --- High Priority Errors ---
    # APK Version Format Error (Error 99 from mkpkg)
    apk_version_error_match = re.search(
        r"ERROR: info field 'version' has invalid value: package version is invalid.*?make\[\d+\]: \*\*\* .*? ([^ ]+\.apk)\] Error 99",
        log_content, re.DOTALL
    )
    if apk_version_error_match:
        apk_filename = os.path.basename(apk_version_error_match.group(1))
        pkg_name_match = re.match(r'^([a-zA-Z0-9._-]+?)(?:=[\d.-]+)?(?:_\d+)?\.apk$', apk_filename) # Improved regex for name
        pkg_name = pkg_name_match.group(1) if pkg_name_match else "unknown_pkg_from_apk"
        # Try to get package name from "Leaving directory" as fallback
        leaving_dir_match = re.search(r"make\[\d+\]: Leaving directory .*?/([^/']+)'", log_content)
        if leaving_dir_match and pkg_name == "unknown_pkg_from_apk":
             pkg_name = leaving_dir_match.group(1)
        return f"apk_invalid_version_format:{pkg_name}"

    # APK Add Invalid Dependency Format (Error 99 from apk add) - often base-files
    apk_add_invalid_format_match = re.search(
        r"ERROR: ('([^=]+)=' is not a valid world dependency).*?make\[\d+\]: \*\*\* .*?package/install.* Error 99",
        log_content, re.DOTALL
    )
    if apk_add_invalid_format_match:
        invalid_package = apk_add_invalid_format_match.group(2)
        if "base-files=" in apk_add_invalid_format_match.group(1):
             return "apk_add_base_files" # Specific signature for base-files issue
        else:
             return f"apk_add_invalid_dep_format:{invalid_package}"

    # Out Of Memory (OOM)
    if re.search(r'Killed|signal 9|Error 137', log_content): return "oom_detected"

    # Filesystem Conflicts
    if "mkdir: cannot create directory" in log_content and "File exists" in log_content: return "directory_conflict"
    if "ln: failed to create symbolic link" in log_content and "File exists" in log_content: return "symlink_conflict" # <-- Your specific error

    # Patching Failures
    if ("Patch failed" in log_content or "Only garbage was found" in log_content or "unexpected end of file in patch" in log_content or "can't find file to patch" in log_content):
         patch_match = re.search(r'Applying (.+\.patch)', log_content)
         patch = os.path.basename(patch_match.group(1)) if patch_match else "unknown_patch"
         pkg_match = re.search(r"make\[\d+\]: Entering directory .*?/([^/']+)", log_content)
         if not pkg_match:
             pkg_match = re.search(r"ERROR: package/(?:feeds/[^/]+/|pkgs/|libs/|utils/|network/)?([^/]+) failed to build", log_content)
         pkg_name = pkg_match.group(1) if pkg_match else "unknown_pkg"
         return f"patch_failed:{pkg_name}:{patch}"

    # Makefile Syntax Errors
    if "missing separator" in log_content and ("Stop." in log_content or "***" in log_content):
         makefile_match = re.search(r'^([^:]+):\d+: \*\*\* missing separator', log_content, re.MULTILINE)
         makefile = makefile_match.group(1) if makefile_match else "unknown_makefile"
         return f"makefile_separator:{makefile}"

    # Toolchain Provides Syntax Error (trailing space)
    if "toolchain" in log_content and 'provides' in log_content and 'syntax error' in log_content and '--info "provides:' in log_content:
        return "toolchain_provides_syntax"

    # APK Wrapper Syntax Error
    if "Syntax error:" in log_content and "bin/apk" in log_content and "staging_dir/host/bin/apk" in log_content:
         return "apk_wrapper_syntax"

    # --- Specific Package/Linker Errors ---
    # Netifd linking error (missing libnl-tiny)
    if "undefined reference to" in log_content and re.search(r'netifd|toolchain.*netifd', log_content) and 'nl_' in log_content:
        ref_match = re.search(r"undefined reference to `([^']+)'", log_content)
        ref = ref_match.group(1) if ref_match else "unknown_symbol"
        return f"netifd_link_error:{ref}"

    # Lua-neturl download failure
    if LIBS_AVAILABLE and 'lua-neturl' in log_content and ('Download failed' in log_content or 'Hash mismatch' in log_content or 'No more mirrors to try' in log_content):
        return "lua_neturl_download"

    # Trojan-plus specific build error
    if 'trojan-plus' in log_content and ('buffer-cast' in log_content or 'std::span' in log_content): # Broaden trigger slightly
        return "trojan_plus_build_error"

    # Luci-lib-taskd specific dependency issue (often manifests as Error 1 or Error 99)
    if ('luci-lib-taskd' in log_content or 'taskd' in log_content) and ('Error 1' in log_content or 'Error 99' in log_content) and ('apk' in log_content or 'depends' in log_content):
        return "luci_lib_taskd_depends"

    # --- Lower Priority / More General Errors ---
    # Makefile Dependency Warning (missing package)
    dep_warning_match = re.search(r"WARNING: Makefile '([^']+)' has a dependency on '([^']*)', which does not exist", log_content)
    if dep_warning_match:
        makefile_path_str = dep_warning_match.group(1)
        bad_dep = dep_warning_match.group(2)
        # Filter out common noisy/ignorable warnings
        if bad_dep and bad_dep.lower() not in ['perl_tests', ''] and not bad_dep.startswith(('p,', '(virtual)', '$', 'gst1-mod-')) and '=>' not in bad_dep:
             try:
                pkg_name = Path(makefile_path_str).parent.name
             except Exception:
                pkg_name = "unknown_pkg"
             return f"makefile_dep_missing:{pkg_name}:{bad_dep}" # Return this only if no higher priority error found

    # Generic Build Fail (if specific package failed message exists)
    generic_fail_match = re.search(r"ERROR: package/(?:feeds/[^/]+/|pkgs/|libs/|utils/|network/)?([^/]+) failed to build", log_content)
    if generic_fail_match:
        return f"generic_build_fail:{generic_fail_match.group(1)}"

    # Generic Error (lowest priority catch-all)
    generic_error_match = re.search(r'(error:|failed|fatal error:|collect2: error: ld returned 1 exit status)', log_content, re.IGNORECASE)
    if generic_error_match:
        error_keyword = generic_error_match.group(1).lower().split(':')[0].replace(' ', '_')
        context_line = ""
        for line in reversed(log_content.splitlines()):
             if generic_error_match.group(1).lower() in line.lower():
                 context_line = re.sub(r'\x1b\[[0-9;]*[mK]', '', line).strip() # Remove ANSI codes
                 context_line = re.sub(r'[^a-zA-Z0-9\s\._\-\+=:/]', '', context_line)[:80] # Keep relevant chars, allow path separators
                 break
        return f"generic_error:{error_keyword}:{context_line}"

    return "unknown_error"


# --- OOM Handling ---
def handle_oom(current_jobs, log_content):
    """Adjusts job count on OOM error."""
    for pattern in OOM_PRONE_PACKAGE_PATTERNS:
        if re.search(pattern, log_content):
            print("æ£€æµ‹åˆ° OOM é«˜é£é™©åŒ…ï¼Œå¼ºåˆ¶ä½¿ç”¨ -j1")
            return 1
    new_jobs = max(1, current_jobs // 2)
    print(f"æ£€æµ‹åˆ° OOMï¼Œå‡å°‘å¹¶è¡Œä»»åŠ¡æ•°: {current_jobs} -> {new_jobs}")
    return new_jobs



def fix_symbolic_link_conflict(log_content):
    """ä¿®å¤ç¬¦å·é“¾æ¥å†²çª (ln: failed to create symbolic link ...: File exists)"""
    print("ğŸ”§ æ£€æµ‹åˆ°ç¬¦å·é“¾æ¥å†²çªï¼Œå°è¯•ä¿®å¤...")
    conflict_match = re.search(r'ln: failed to create symbolic link [\'"]?([^\'"]+)[\'"]?: File exists', log_content)
    if not conflict_match:
        print("â„¹ï¸ æœªåŒ¹é…åˆ° 'File exists' ç¬¦å·é“¾æ¥å†²çªæ—¥å¿—ã€‚")
        return False

    conflict_link_str = conflict_match.group(1).strip()
    conflict_link = Path(conflict_link_str)
    conflict_link_rel = get_relative_path(conflict_link_str) # For logging
    print(f"å†²çªç¬¦å·é“¾æ¥è·¯å¾„: {conflict_link_rel}")

    # Safety check
    critical_dirs = [Path.cwd(), Path.home(), Path("/"), Path("~"), Path("."), Path("..")]
    try:
        resolved_path = conflict_link.resolve()
        if resolved_path in [p.resolve() for p in critical_dirs if p.exists()] or not conflict_link_str:
            print(f"âŒ æ£€æµ‹åˆ°å…³é”®ç›®å½•æˆ–æ— æ•ˆè·¯å¾„ ({conflict_link_rel})ï¼Œæ‹’ç»åˆ é™¤ï¼")
            return False
    except Exception: # Handle cases where resolve might fail (e.g., broken link)
        pass

    if conflict_link.exists() or conflict_link.is_symlink(): # Check existence or if it's a broken symlink
        print(f"å°è¯•åˆ é™¤å·²å­˜åœ¨çš„æ–‡ä»¶/ç›®å½•/é“¾æ¥: {conflict_link_rel}")
        try:
            if conflict_link.is_dir() and not conflict_link.is_symlink():
                 shutil.rmtree(conflict_link)
                 print(f"âœ… æˆåŠŸåˆ é™¤å†²çªç›®å½• {conflict_link_rel}ã€‚")
            else:
                 conflict_link.unlink() # Works for files and symlinks (including broken ones)
                 print(f"âœ… æˆåŠŸåˆ é™¤å†²çªæ–‡ä»¶/é“¾æ¥ {conflict_link_rel}ã€‚")
            return True
        except Exception as e:
            print(f"âŒ åˆ é™¤ {conflict_link_rel} å¤±è´¥: {e}")
            return False
    else:
        print(f"â„¹ï¸ å†²çªé“¾æ¥è·¯å¾„ {conflict_link_rel} å½“å‰ä¸å­˜åœ¨ï¼Œå¯èƒ½å·²è¢«å¤„ç†ã€‚")
        return True # Conflict resolved

# --- Placeholder for other fix functions ---
# Add all your other fix functions here...
def fix_netifd_libnl_tiny():
    """å¢å¼ºç‰ˆï¼šä¿®å¤ netifd ç¼–è¯‘æ—¶ç¼ºå°‘ libnl-tiny çš„é“¾æ¥é—®é¢˜"""
    import glob

    print("ğŸ”§ æ­£åœ¨å°è¯•ä¿®å¤ netifd ç¼ºå°‘ libnl-tiny çš„é“¾æ¥é”™è¯¯...")
    fixed = False

    try:
        # --- å¼ºåˆ¶æ¸…ç† ---
        print("ğŸ§¹ å¼ºåˆ¶æ¸…ç† libnl-tiny å’Œ netifd...")
        subprocess.run(["make", "package/libs/libnl-tiny/clean", "V=s"], check=False, capture_output=True)
        subprocess.run(["make", "package/network/config/netifd/clean", "V=s"], check=False, capture_output=True)
        # æ¸…ç† netifd çš„ CMake ç¼“å­˜ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        cmake_cache_files = glob.glob("build_dir/target-*/netifd-*/CMakeCache.txt")
        for cache_file in cmake_cache_files:
            print(f"ğŸ—‘ï¸ åˆ é™¤ CMake ç¼“å­˜: {get_relative_path(cache_file)}")
            try:
                os.remove(cache_file)
            except OSError as e:
                print(f"è­¦å‘Š: åˆ é™¤ CMake ç¼“å­˜å¤±è´¥: {e}")


        # --- é‡æ–°ç¼–è¯‘ libnl-tiny ---
        print("ğŸ”¨ ç¼–è¯‘ libnl-tiny...")
        compile_result = subprocess.run(["make", "package/libs/libnl-tiny/compile", "V=s"], check=False, capture_output=True, text=True)
        if compile_result.returncode != 0:
            print(f"âŒ libnl-tiny ç¼–è¯‘å¤±è´¥:\n{compile_result.stderr[-500:]}")
            # return False # ä¸è¦ç«‹å³è¿”å›ï¼Œç»§ç»­å°è¯•ä¿®æ”¹ netifd

        print("ğŸ“¦ å®‰è£… libnl-tiny...")
        install_result = subprocess.run(["make", "package/libs/libnl-tiny/install", "V=s"], check=False, capture_output=True, text=True)
        if install_result.returncode != 0:
            print(f"âŒ libnl-tiny å®‰è£…å¤±è´¥:\n{install_result.stderr[-500:]}")
            # return False

        # --- ç¡®è®¤ libnl-tiny åº“æ–‡ä»¶ ---
        lib_paths = glob.glob("staging_dir/target-*/usr/lib/libnl-tiny.so") # ä¼˜å…ˆæ£€æŸ¥ .so
        if not lib_paths:
             lib_paths = glob.glob("staging_dir/target-*/usr/lib/libnl-tiny.a") # æ£€æŸ¥ .a
        if not lib_paths:
            print("âŒ æœªæ‰¾åˆ° libnl-tiny çš„åº“æ–‡ä»¶ (libnl-tiny.so æˆ– libnl-tiny.a)ï¼Œä¿®å¤å¯èƒ½æ— æ•ˆã€‚")
            # return False # å³ä½¿æ‰¾ä¸åˆ°ä¹Ÿå¯èƒ½é€šè¿‡åç»­æ­¥éª¤ä¿®å¤
        else:
            print(f"âœ… æ‰¾åˆ° libnl-tiny åº“æ–‡ä»¶: {get_relative_path(lib_paths[0])}")

        # --- ä¿®æ”¹ netifd çš„ Makefile ---
        netifd_makefile = Path("package/network/config/netifd/Makefile")
        if netifd_makefile.exists():
            print(f"ğŸ”§ æ£€æŸ¥å¹¶ä¿®æ”¹ {get_relative_path(str(netifd_makefile))}...")
            content_changed = False
            with open(netifd_makefile, "r", encoding="utf-8") as f:
                lines = f.readlines()

            new_lines = []
            depends_found = False
            ldflags_found = False
            for line in lines:
                if line.strip().startswith("DEPENDS:="):
                    depends_found = True
                    if "+libnl-tiny" not in line:
                        print("  â• æ·»åŠ  +libnl-tiny åˆ° DEPENDS")
                        line = line.rstrip() + " +libnl-tiny\n"
                        content_changed = True
                elif line.strip().startswith("TARGET_LDFLAGS +="):
                     ldflags_found = True
                     if "-lnl-tiny" not in line:
                         print("  â• æ·»åŠ  -lnl-tiny åˆ° TARGET_LDFLAGS")
                         line = line.rstrip() + " -lnl-tiny\n"
                         content_changed = True
                new_lines.append(line)

            # å¦‚æœæ²¡æœ‰æ‰¾åˆ° TARGET_LDFLAGSï¼Œåˆ™åœ¨ PKG_BUILD_DEPENDS åæ·»åŠ 
            if not ldflags_found:
                 try:
                     insert_index = next(i for i, line in enumerate(new_lines) if line.strip().startswith('PKG_BUILD_DEPENDS:=')) + 1
                     print("  â• æ·»åŠ  TARGET_LDFLAGS += -lnl-tiny")
                     new_lines.insert(insert_index, 'TARGET_LDFLAGS += -lnl-tiny\n')
                     content_changed = True
                 except StopIteration:
                     print("  âš ï¸ æœªæ‰¾åˆ° PKG_BUILD_DEPENDSï¼Œæ— æ³•è‡ªåŠ¨æ·»åŠ  TARGET_LDFLAGS")


            if content_changed:
                with open(netifd_makefile, "w", encoding="utf-8") as f:
                    f.writelines(new_lines)
                print(f"âœ… å·²ä¿®æ”¹ {get_relative_path(str(netifd_makefile))}")
                fixed = True
            else:
                print(f"â„¹ï¸ {get_relative_path(str(netifd_makefile))} æ— éœ€ä¿®æ”¹ã€‚")
        else:
            print(f"âš ï¸ æœªæ‰¾åˆ° {get_relative_path(str(netifd_makefile))}")

        # --- ä¿®æ”¹ netifd çš„ CMakeLists.txt (ä½œä¸ºè¡¥å……) ---
        # CMake é€šå¸¸ä¼šé€šè¿‡ DEPENDS è‡ªåŠ¨æ‰¾åˆ°åº“ï¼Œä½†ä»¥é˜²ä¸‡ä¸€
        cmake_path = Path("package/network/config/netifd/CMakeLists.txt")
        if cmake_path.exists():
            print(f"ğŸ”§ æ£€æŸ¥å¹¶ä¿®æ”¹ {get_relative_path(str(cmake_path))}...")
            content_changed = False
            with open(cmake_path, "r", encoding="utf-8") as f:
                content = f.read()

            # æŸ¥æ‰¾ target_link_libraries(netifd ...)
            link_match = re.search(r"target_link_libraries\s*\(\s*netifd\s+([^\)]+)\)", content, re.IGNORECASE)
            if link_match:
                linked_libs = link_match.group(1)
                if 'nl-tiny' not in linked_libs and 'libnl-tiny' not in linked_libs:
                    print("  â• æ·»åŠ  nl-tiny åˆ° target_link_libraries")
                    new_content = content.replace(
                        link_match.group(0),
                        f"target_link_libraries(netifd nl-tiny {linked_libs.strip()})"
                    )
                    content_changed = True
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•åœ¨ add_executable åæ·»åŠ 
            elif "add_executable(netifd" in content and "target_link_libraries(netifd" not in content:
                 print("  â• æ·»åŠ æ–°çš„ target_link_libraries(netifd nl-tiny ...)")
                 # å°è¯•æ‰¾åˆ°å·²æœ‰çš„åº“ä¾èµ–ï¼ˆé€šå¸¸æ˜¯ ubox, ubus ç­‰ï¼‰
                 existing_libs = []
                 find_lib_matches = re.findall(r"find_package\(([^ ]+)\s+REQUIRED\)", content)
                 if find_lib_matches:
                     existing_libs = [f"${{{lib.upper()}_LIBRARIES}}" for lib in find_lib_matches]
                 # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå°±ç”¨å·²çŸ¥çš„åŸºç¡€åº“
                 if not existing_libs:
                     existing_libs = ["${UBOX_LIBRARIES}", "${UBUS_LIBRARIES}", "${UCI_LIBRARIES}", "${JSONC_LIBRARIES}", "${BLOBMSG_JSON_LIBRARIES}"] # å¯èƒ½éœ€è¦è°ƒæ•´

                 new_content = re.sub(
                     r"(add_executable\(netifd[^\)]+\))",
                     r"\1\ntarget_link_libraries(netifd nl-tiny " + " ".join(existing_libs) + ")",
                     content,
                     count=1
                 )
                 content_changed = True


            if content_changed:
                with open(cmake_path, "w", encoding="utf-8") as f:
                    f.write(new_content)
                print(f"âœ… å·²ä¿®æ”¹ {get_relative_path(str(cmake_path))}")
                fixed = True
            else:
                print(f"â„¹ï¸ {get_relative_path(str(cmake_path))} æ— éœ€ä¿®æ”¹ã€‚")
        else:
            print(f"âš ï¸ æœªæ‰¾åˆ° {get_relative_path(str(cmake_path))}")


        # --- å†æ¬¡æ¸…ç† netifd ä»¥ç¡®ä¿æ›´æ”¹ç”Ÿæ•ˆ ---
        if fixed:
            print("ğŸ§¹ å†æ¬¡æ¸…ç† netifd ä»¥åº”ç”¨æ›´æ”¹...")
            subprocess.run(["make", "package/network/config/netifd/clean", "V=s"], check=False, capture_output=True)

        print("âœ… netifd å’Œ libnl-tiny ä¿®å¤æµç¨‹å®Œæˆã€‚")
        # å³ä½¿æ²¡æœ‰æ˜ç¡®ä¿®æ”¹æ–‡ä»¶ï¼Œä¹Ÿè¿”å› Trueï¼Œå› ä¸ºæ¸…ç†å’Œé‡æ–°ç¼–è¯‘æœ¬èº«å°±æ˜¯ä¸€ç§ä¿®å¤å°è¯•
        return True

    except Exception as e:
        print(f"âŒ ä¿®å¤ netifd/libnl-tiny æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
        return False

def fix_trojan_plus_issues():
    """ä¿®å¤ trojan-plus ç›¸å…³çš„ç¼–è¯‘é—®é¢˜"""
    print("ğŸ”§ æ£€æµ‹åˆ° trojan-plus ç›¸å…³é”™è¯¯ï¼Œå°è¯•ç¦ç”¨...")
    makefile_paths = list(Path(".").glob("**/luci-app-passwall/Makefile"))
    fixed_any = False
    for makefile_path in makefile_paths:
        try:
            print(f"æ£€æŸ¥: {get_relative_path(str(makefile_path))}")
            with open(makefile_path, "r", encoding="utf-8", errors="replace") as f:
                content = f.read()
            original_content = content

            # ç¦ç”¨ select PACKAGE_trojan-plus
            content = re.sub(r'^\s*\+\s*PACKAGE_trojan-plus\s*.*?\n', '', content, flags=re.MULTILINE)
            # ç¦ç”¨ default y for Trojan_Plus include
            content = re.sub(r'(config PACKAGE_.*?_INCLUDE_Trojan_Plus\s*\n(?:.*\n)*?\s*default )\s*y', r'\1n', content)

            if content != original_content:
                with open(makefile_path, "w", encoding="utf-8") as f:
                    f.write(content)
                print(f"âœ… å·²ä¿®æ”¹ {get_relative_path(str(makefile_path))}")
                fixed_any = True
            else:
                print(f"â„¹ï¸ {get_relative_path(str(makefile_path))} æ— éœ€ä¿®æ”¹ã€‚")

        except Exception as e:
            print(f"âŒ å¤„ç† {get_relative_path(str(makefile_path))} æ—¶å‡ºé”™: {e}")

    if fixed_any:
        # æ¸…ç† trojan-plus åŒ…ä»¥ç¡®ä¿ä¿®æ”¹ç”Ÿæ•ˆ
        print("ğŸ§¹ æ¸…ç† trojan-plus ç›¸å…³åŒ…...")
        # Find the package path dynamically
        trojan_plus_paths = list(Path(".").glob("**/trojan-plus/Makefile"))
        for tp_path in trojan_plus_paths:
            try:
                pkg_path = tp_path.parent.relative_to(Path.cwd())
                clean_cmd = ["make", f"{pkg_path}/clean", "V=s"]
                print(f"è¿è¡Œ: {' '.join(clean_cmd)}")
                subprocess.run(clean_cmd, check=False, capture_output=True)
            except ValueError:
                print(f"âš ï¸ æ— æ³•è·å– {tp_path.parent} çš„ç›¸å¯¹è·¯å¾„è¿›è¡Œæ¸…ç†ã€‚")
            except Exception as e:
                print(f"âš ï¸ æ‰§è¡Œæ¸…ç†å‘½ä»¤æ—¶å‡ºé”™: {e}")
        return True
    else:
        print("â„¹ï¸ æœªæ‰¾åˆ°éœ€è¦ä¿®å¤çš„ trojan-plus ç›¸å…³ Makefileã€‚")
        return False


def fix_lua_neturl_directory():
    """ä¿®å¤ lua-neturl çš„ Makefile å’Œè¡¥ä¸"""
    print("ğŸ”§ ä¿®å¤ lua-neturl Makefile å’Œè¡¥ä¸...")
    makefile_path_pattern = "**/lua-neturl/Makefile"
    makefile_paths = list(Path(".").glob(makefile_path_pattern))

    if not makefile_paths:
        print("âŒ æ— æ³•æ‰¾åˆ° lua-neturl çš„ Makefile")
        return False

    makefile_path = makefile_paths[0] # Assume first found is the correct one
    patch_dir = makefile_path.parent / "patches"
    print(f"æ‰¾åˆ° Makefile: {get_relative_path(str(makefile_path))}")
    modified = False

    try:
        with open(makefile_path, 'r', encoding='utf-8', errors='replace') as f:
            content = f.read()
        original_content = content

        # ç¡®ä¿ PKG_BUILD_DIR æ­£ç¡®
        pkg_source_match = re.search(r'^\s*PKG_SOURCE:=([^\n]+)', content, re.MULTILINE)
        pkg_version_match = re.search(r'^\s*PKG_VERSION:=([^\n]+)', content, re.MULTILINE)
        pkg_release_match = re.search(r'^\s*PKG_RELEASE:=([^\n]+)', content, re.MULTILINE)

        if pkg_source_match and pkg_version_match:
            pkg_source = pkg_source_match.group(1).strip()
            pkg_version = pkg_version_match.group(1).strip()
            pkg_release = pkg_release_match.group(1).strip() if pkg_release_match else "1"

            # Derive expected dir name, e.g., neturl-1.2 or neturl-v1.2-1
            expected_subdir = f"neturl-{pkg_version}"
            if pkg_release and pkg_release != "1":
                 # Check if version already contains release-like suffix
                 if not pkg_version.endswith(f"-{pkg_release}"):
                      expected_subdir += f"-{pkg_release}" # Less common but possible

            # More robust: look at PKG_SOURCE name pattern like neturl-xxx.tar.gz
            source_base = Path(pkg_source).stem
            if source_base.endswith('.tar'): # Handle .tar.gz etc.
                source_base = Path(source_base).stem
            if source_base.startswith("neturl-"):
                expected_subdir = source_base
            elif source_base.startswith("v"): # Handle tags like v1.2-1
                 expected_subdir = f"neturl-{source_base.lstrip('v')}"


            build_dir_line = f"PKG_BUILD_DIR:=$(BUILD_DIR)/{expected_subdir}"
            build_dir_regex = r'^\s*PKG_BUILD_DIR:=\$\(BUILD_DIR\)/.*'

            if not re.search(build_dir_regex, content, re.MULTILINE):
                # Insert after PKG_SOURCE_URL or PKG_HASH
                insert_after = r'^\s*PKG_HASH:=[^\n]+'
                if not re.search(insert_after, content, re.MULTILINE):
                    insert_after = r'^\s*PKG_SOURCE_URL:=[^\n]+'
                if not re.search(insert_after, content, re.MULTILINE):
                     insert_after = r'^\s*PKG_RELEASE:=[^\n]+' # Fallback

                if re.search(insert_after, content, re.MULTILINE):
                     content = re.sub(f'({insert_after})', f'\\1\n{build_dir_line}', content, 1, re.MULTILINE)
                     print(f"âœ… æ·»åŠ  PKG_BUILD_DIR: {build_dir_line}")
                     modified = True
                else:
                     print("âš ï¸ æ— æ³•æ‰¾åˆ°åˆé€‚çš„æ’å…¥ç‚¹æ¥æ·»åŠ  PKG_BUILD_DIR")

            elif not re.search(r'^\s*PKG_BUILD_DIR:=\$\(BUILD_DIR\)/' + re.escape(expected_subdir) + r'\s*$', content, re.MULTILINE):
                 content = re.sub(build_dir_regex, build_dir_line, content, 1, re.MULTILINE)
                 print(f"âœ… ä¿®æ­£ PKG_BUILD_DIR ä¸º: {build_dir_line}")
                 modified = True

        else:
            print("âš ï¸ æ— æ³•ä» Makefile ä¸­æå– PKG_SOURCE æˆ– PKG_VERSIONã€‚")

        if content != original_content:
            with open(makefile_path, 'w', encoding='utf-8') as f:
                f.write(content)

        # å¤„ç†è¡¥ä¸ç›®å½• (éš”ç¦»é .patch æ–‡ä»¶)
        if patch_dir.exists() and patch_dir.is_dir():
            excluded_dir = patch_dir / "excluded"
            excluded_dir.mkdir(exist_ok=True)
            for item in patch_dir.iterdir():
                if item.is_file() and not item.name.endswith('.patch') and item.name != "excluded":
                    try:
                        dest = excluded_dir / item.name
                        shutil.move(str(item), str(dest))
                        print(f"âœ… å·²éš”ç¦»æ— æ•ˆè¡¥ä¸æ–‡ä»¶: {item.name} -> excluded/")
                        modified = True
                    except Exception as e:
                        print(f"âŒ éš”ç¦»æ–‡ä»¶ {item.name} å¤±è´¥: {e}")

    except Exception as e:
        print(f"âŒ å¤„ç† lua-neturl Makefile æ—¶å‡ºé”™: {e}")
        return False

    if modified:
        print("âœ… å·²å®Œæˆ lua-neturl çš„ Makefile å’Œè¡¥ä¸ä¿®å¤ã€‚")
        # Clean the package to apply changes
        try:
            pkg_rel_path = makefile_path.parent.relative_to(Path.cwd())
            subprocess.run(["make", f"{pkg_rel_path}/clean", "V=s"], check=False, capture_output=True)
        except ValueError:
            print(f"âš ï¸ æ— æ³•è·å– {makefile_path.parent} çš„ç›¸å¯¹è·¯å¾„è¿›è¡Œæ¸…ç†ã€‚")
        except Exception as e:
            print(f"âš ï¸ æ‰§è¡Œæ¸…ç†å‘½ä»¤æ—¶å‡ºé”™: {e}")
        return True
    else:
        print("â„¹ï¸ lua-neturl æ— éœ€ä¿®å¤ã€‚")
        return False


def fix_patch_application(log_content):
    """ä¿®å¤è¡¥ä¸åº”ç”¨å¤±è´¥çš„é—®é¢˜"""
    print("ğŸ”§ æ£€æµ‹åˆ°è¡¥ä¸åº”ç”¨å¤±è´¥ï¼Œå°è¯•ä¿®å¤...")

    patch_failed_regex = r'Applying (.*?)(?: to .*)? using plaintext.*\n(?:.*\n){0,5}?(?:patch unexpectedly ends|Only garbage found|can\'t find file to patch|Hunk #\d+ FAILED)'
    patch_match = re.search(patch_failed_regex, log_content, re.MULTILINE)

    if not patch_match:
        print("â„¹ï¸ æœªæ˜ç¡®åŒ¹é…åˆ°è¡¥ä¸å¤±è´¥æ—¥å¿—ã€‚")
        return False

    patch_file_str = patch_match.group(1).strip()
    patch_file_path = Path(patch_file_str)
    patch_file_rel = get_relative_path(patch_file_str) # For logging
    print(f"è¯†åˆ«åˆ°å¯èƒ½å¤±è´¥çš„è¡¥ä¸æ–‡ä»¶: {patch_file_rel}")

    if not patch_file_path.exists():
         # Try to find it relative to CWD if it's not absolute
         patch_file_path_abs = Path.cwd() / patch_file_str
         if patch_file_path_abs.exists():
             patch_file_path = patch_file_path_abs
             patch_file_rel = get_relative_path(str(patch_file_path_abs)) # Update relative path
         else:
             print(f"âŒ è¡¥ä¸æ–‡ä»¶ {patch_file_rel} æœªæ‰¾åˆ°ï¼Œæ— æ³•ä¿®å¤ã€‚")
             return False

    # Specific fix for lua-neturl patch issues
    if "lua-neturl" in str(patch_file_path):
        print("æ£€æµ‹åˆ° lua-neturl è¡¥ä¸å¤±è´¥ï¼Œè°ƒç”¨ä¸“ç”¨ä¿®å¤å‡½æ•°...")
        return fix_lua_neturl_directory() # This function handles both Makefile and patches

    # General fix: try removing the problematic patch
    print(f"è¡¥ä¸åº”ç”¨å¤±è´¥ï¼Œå°è¯•ç§»é™¤è¡¥ä¸æ–‡ä»¶: {patch_file_rel}")
    try:
        # Backup first
        backup_path = patch_file_path.with_suffix(patch_file_path.suffix + ".disabled")
        shutil.move(str(patch_file_path), str(backup_path))
        print(f"âœ… å·²ç¦ç”¨è¡¥ä¸æ–‡ä»¶ (é‡å‘½åä¸º {backup_path.name})ã€‚")

        # Attempt to clean the package the patch belongs to
        # Try to guess package path from patch path (e.g., feeds/xxx/pkg/patches/ -> feeds/xxx/pkg)
        try:
            pkg_dir = patch_file_path.parent.parent # Go up from /patches
            if pkg_dir.exists() and (pkg_dir / "Makefile").exists():
                 pkg_rel_path = get_relative_path(str(pkg_dir))
                 print(f"ğŸ§¹ å°è¯•æ¸…ç†ç›¸å…³åŒ…: {pkg_rel_path}")
                 subprocess.run(["make", f"{pkg_rel_path}/clean", "V=s"], check=False, capture_output=True)
            else:
                 print("âš ï¸ æ— æ³•ç¡®å®šè¡¥ä¸æ‰€å±åŒ…ç›®å½•ï¼Œè·³è¿‡æ¸…ç†ã€‚")
        except Exception as clean_e:
            print(f"âš ï¸ æ¸…ç†åŒ…æ—¶å‡ºé”™: {clean_e}")

        return True
    except Exception as e:
        print(f"âŒ ç¦ç”¨è¡¥ä¸ {patch_file_rel} å¤±è´¥: {e}")
        return False


def fix_makefile_separator(log_content):
    """ä¿®å¤ Makefile "missing separator" é”™è¯¯"""
    print("ğŸ”§ æ£€æµ‹åˆ° 'missing separator' é”™è¯¯ï¼Œå°è¯•ä¿®å¤...")
    fixed = False

    error_line_match = re.search(r'^([\/\w\.\-]+):(\d+):\s+\*\*\*\s+missing separator', log_content, re.MULTILINE)

    if not error_line_match:
        print("âš ï¸ æ— æ³•ä»æ—¥å¿—ä¸­ç²¾ç¡®æå–æ–‡ä»¶åå’Œè¡Œå·ã€‚")
        return False

    makefile_name_from_err = error_line_match.group(1)
    line_num = int(error_line_match.group(2))
    print(f"è¯†åˆ«åˆ°é”™è¯¯ä½ç½®: æ–‡ä»¶='{makefile_name_from_err}', è¡Œå·={line_num}")

    # Try to find the context directory from "make[X]: Entering directory ..." lines above the error
    log_lines = log_content.splitlines()
    error_line_index = -1
    for i, line in enumerate(log_lines):
        if error_line_match.group(0) in line:
            error_line_index = i
            break

    context_dir = Path.cwd() # Default to current dir
    if error_line_index != -1:
        for i in range(error_line_index - 1, max(0, error_line_index - 50), -1):
            dir_match = re.search(r"make\[\d+\]: Entering directory '([^']+)'", log_lines[i])
            if dir_match:
                # Resolve potential relative paths from log
                potential_dir = Path(dir_match.group(1))
                if potential_dir.is_dir():
                    context_dir = potential_dir.resolve() # Use resolved absolute path
                    print(f"æ‰¾åˆ°ä¸Šä¸‹æ–‡ç›®å½•: {get_relative_path(str(context_dir))}")
                    break
                else: # If log path is not absolute, try relative to CWD
                    potential_dir = Path.cwd() / dir_match.group(1)
                    if potential_dir.is_dir():
                        context_dir = potential_dir.resolve()
                        print(f"æ‰¾åˆ°ä¸Šä¸‹æ–‡ç›®å½• (ç›¸å¯¹è§£æ): {get_relative_path(str(context_dir))}")
                        break

    # Construct absolute path to the makefile
    makefile_path = (context_dir / makefile_name_from_err).resolve()
    makefile_path_rel = get_relative_path(str(makefile_path)) # For display

    print(f"å°è¯•ä¿®å¤æ–‡ä»¶: {makefile_path_rel}")

    if makefile_path.is_file():
        try:
            with open(makefile_path, 'r', encoding='utf-8', errors='replace') as f:
                makefile_lines = f.readlines()

            if 0 < line_num <= len(makefile_lines):
                line_content = makefile_lines[line_num - 1]

                if re.match(r'^[ ]+', line_content) and not line_content.startswith('\t'):
                    print(f"æ£€æµ‹åˆ°ç¬¬ {line_num} è¡Œä½¿ç”¨ç©ºæ ¼ç¼©è¿›ï¼Œæ›¿æ¢ä¸º TAB...")
                    backup_path = makefile_path.with_suffix(makefile_path.suffix + ".bak")
                    try:
                        shutil.copy2(makefile_path, backup_path)
                        print(f"åˆ›å»ºå¤‡ä»½: {get_relative_path(str(backup_path))}")
                    except Exception as backup_e:
                        print(f"âš ï¸ åˆ›å»ºå¤‡ä»½å¤±è´¥: {backup_e}")
                        backup_path = None # Indicate backup failed

                    makefile_lines[line_num - 1] = '\t' + line_content.lstrip(' ')
                    with open(makefile_path, 'w', encoding='utf-8') as f:
                        f.writelines(makefile_lines)

                    # Verify fix
                    with open(makefile_path, 'r', encoding='utf-8', errors='replace') as f_check:
                         fixed_lines = f_check.readlines()
                    if fixed_lines[line_num - 1].startswith('\t'):
                         print(f"âœ… æˆåŠŸä¿®å¤ç¬¬ {line_num} è¡Œç¼©è¿›ã€‚")
                         fixed = True
                         if backup_path and backup_path.exists(): os.remove(backup_path) # Remove backup on success
                    else:
                         print(f"âŒ ä¿®å¤å¤±è´¥ï¼Œç¬¬ {line_num} è¡Œå†…å®¹ä»ä¸º: '{fixed_lines[line_num-1].rstrip()}'")
                         if backup_path and backup_path.exists():
                             shutil.move(str(backup_path), makefile_path) # Restore backup
                             print("å·²æ¢å¤å¤‡ä»½ã€‚")

                elif not line_content.strip() and line_content != '\n':
                     print(f"ç¬¬ {line_num} è¡Œä¸ºéæ ‡å‡†ç©ºè¡Œï¼Œå°è¯•è§„èŒƒåŒ–ä¸ºç©ºè¡Œ...")
                     backup_path = makefile_path.with_suffix(makefile_path.suffix + ".bak")
                     try:
                         shutil.copy2(makefile_path, backup_path)
                     except Exception: backup_path = None

                     makefile_lines[line_num - 1] = '\n'
                     with open(makefile_path, 'w', encoding='utf-8') as f:
                         f.writelines(makefile_lines)
                     print("âœ… å·²è§„èŒƒåŒ–ç©ºè¡Œã€‚")
                     fixed = True
                     if backup_path and backup_path.exists(): os.remove(backup_path)

                else:
                    print(f"â„¹ï¸ ç¬¬ {line_num} è¡Œå†…å®¹: '{line_content.rstrip()}'ã€‚çœ‹èµ·æ¥ä¸æ˜¯ç®€å•çš„ç©ºæ ¼ç¼©è¿›é—®é¢˜ï¼Œå¯èƒ½éœ€è¦æ‰‹åŠ¨æ£€æŸ¥æˆ–é—®é¢˜åœ¨ include çš„æ–‡ä»¶ä¸­ã€‚")

            else:
                print(f"âŒ è¡Œå· {line_num} è¶…å‡ºæ–‡ä»¶ {makefile_path_rel} çš„èŒƒå›´ ({len(makefile_lines)} è¡Œ)ã€‚")

        except Exception as e:
            print(f"âŒ è¯»å†™æ–‡ä»¶ {makefile_path_rel} æ—¶å‡ºé”™: {e}")

    else:
        print(f"âŒ æ–‡ä»¶ '{makefile_path_rel}' ä¸å­˜åœ¨æˆ–ä¸æ˜¯æ–‡ä»¶ã€‚")

    # If a fix was attempted or the error persists, try cleaning the package directory
    if fixed or not fixed: # Always try cleaning if separator error occurred
        pkg_dir = makefile_path.parent
        # Heuristic: Check if the parent dir looks like a package dir
        if pkg_dir.exists() and (pkg_dir / "Makefile").exists() and pkg_dir != Path.cwd():
            try:
                pkg_rel_path = get_relative_path(str(pkg_dir))
                print(f"ğŸ§¹ å°è¯•æ¸…ç†ç›¸å…³åŒ…ç›®å½•: {pkg_rel_path}...")
                # Use DIRCLEAN=1 for a deeper clean
                subprocess.run(["make", f"{pkg_rel_path}/clean", "DIRCLEAN=1", "V=s"], check=False, capture_output=True)
                print(f"âœ… æ¸…ç†å‘½ä»¤å·²æ‰§è¡Œã€‚")
                fixed = True # Indicate an action was taken for this error
            except Exception as e:
                print(f"âš ï¸ æ‰§è¡Œæ¸…ç†å‘½ä»¤æ—¶å‡ºé”™: {e}")
        elif makefile_path.name == "Makefile" and context_dir == Path.cwd():
             print(f"ğŸ§¹ é”™è¯¯å‘ç”Ÿåœ¨æ ¹ Makefileï¼Œå°è¯•æ‰§è¡Œ 'make clean'... (è¿™å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´)")
             try:
                 subprocess.run(["make", "clean", "V=s"], check=False, capture_output=True)
                 print(f"âœ… 'make clean' å‘½ä»¤å·²æ‰§è¡Œã€‚")
                 fixed = True
             except Exception as e:
                 print(f"âš ï¸ æ‰§è¡Œ 'make clean' æ—¶å‡ºé”™: {e}")

    return fixed


def fix_directory_conflict(log_content):
    """ä¿®å¤ç›®å½•å†²çª (mkdir: cannot create directory ...: File exists)"""
    print("ğŸ”§ æ£€æµ‹åˆ°ç›®å½•å†²çªï¼Œå°è¯•ä¿®å¤...")
    conflict_match = re.search(r'mkdir: cannot create directory [\'"]?([^\'"]+)[\'"]?: File exists', log_content)
    if not conflict_match:
        print("â„¹ï¸ æœªåŒ¹é…åˆ° 'File exists' ç›®å½•å†²çªæ—¥å¿—ã€‚")
        return False

    conflict_path_str = conflict_match.group(1).strip()
    conflict_path = Path(conflict_path_str)
    conflict_path_rel = get_relative_path(conflict_path_str) # For logging
    print(f"å†²çªè·¯å¾„: {conflict_path_rel}")

    # Important safety check: Avoid deleting critical directories
    critical_dirs = [Path.cwd(), Path.home(), Path("/"), Path("~"), Path("."), Path("..")]
    try:
        resolved_path = conflict_path.resolve()
        if resolved_path in [p.resolve() for p in critical_dirs if p.exists()] or not conflict_path_str:
            print(f"âŒ æ£€æµ‹åˆ°å…³é”®ç›®å½•æˆ–æ— æ•ˆè·¯å¾„ ({conflict_path_rel})ï¼Œæ‹’ç»åˆ é™¤ï¼")
            return False
    except Exception: # Handle cases where resolve might fail
        pass

    # Check if it's a file or a directory
    if conflict_path.is_file():
        print(f"å†²çªè·¯å¾„æ˜¯ä¸€ä¸ªæ–‡ä»¶ï¼Œå°è¯•åˆ é™¤æ–‡ä»¶: {conflict_path_rel}")
        try:
            conflict_path.unlink()
            print("âœ… æˆåŠŸåˆ é™¤å†²çªæ–‡ä»¶ã€‚")
            return True
        except Exception as e:
            print(f"âŒ åˆ é™¤æ–‡ä»¶ {conflict_path_rel} å¤±è´¥: {e}")
            return False
    elif conflict_path.is_dir():
        print(f"å†²çªè·¯å¾„æ˜¯ä¸€ä¸ªç›®å½•ï¼Œå°è¯•åˆ é™¤ç›®å½•: {conflict_path_rel}")
        try:
            shutil.rmtree(conflict_path)
            print("âœ… æˆåŠŸåˆ é™¤å†²çªç›®å½•ã€‚")
            return True
        except Exception as e:
            print(f"âŒ åˆ é™¤ç›®å½• {conflict_path_rel} å¤±è´¥: {e}")
            return False
    else:
        print(f"â„¹ï¸ å†²çªè·¯å¾„ {conflict_path_rel} å½“å‰ä¸å­˜åœ¨ï¼Œå¯èƒ½å·²è¢«å¤„ç†ã€‚")
        return True # Conflict resolved


def fix_pkg_version_format():
    """ä¿®å¤ PKG_VERSION å’Œ PKG_RELEASE æ ¼å¼ (ç®€å•æ•°å­—æˆ–æ ‡å‡†æ ¼å¼)"""
    print("ğŸ”§ ä¿®å¤ Makefile ä¸­çš„ PKG_VERSION å’Œ PKG_RELEASE æ ¼å¼...")
    changed_count = 0
    makefile_pattern = "**/Makefile" # Look for Makefiles everywhere except build/staging/tmp
    ignore_dirs = ['build_dir', 'staging_dir', 'tmp', '.git', 'dl']

    all_makefiles = []
    for p in Path('.').rglob('Makefile'): # Use rglob for recursive search
        # Check if the path is within an ignored directory
        if not any(ignored in p.parts for ignored in ignore_dirs):
            all_makefiles.append(p)

    print(f"æ‰¾åˆ° {len(all_makefiles)} ä¸ªæ½œåœ¨çš„ Makefile æ–‡ä»¶è¿›è¡Œæ£€æŸ¥...")

    processed_count = 0
    for makefile in all_makefiles:
        processed_count += 1
        if processed_count % 200 == 0: # Adjust reporting frequency if needed
             print(f"å·²æ£€æŸ¥ {processed_count}/{len(all_makefiles)}...")

        try:
            with open(makefile, 'r', encoding='utf-8', errors='replace') as f:
                content = f.read()
            original_content = content
            current_content = content

            # Check if it's an OpenWrt package Makefile (basic check)
            if not ('include $(TOPDIR)/rules.mk' in content or 'include ../../buildinfo.mk' in content or 'include $(INCLUDE_DIR)/package.mk' in content):
                continue

            modified_in_file = False

            # --- Fix PKG_VERSION ---
            version_match = re.search(r'^(PKG_VERSION:=)(.*)$', current_content, re.MULTILINE)
            if version_match:
                current_version_line = version_match.group(0)
                current_version = version_match.group(2).strip()
                # Simple fix: remove leading 'v' if present
                if current_version.startswith('v'):
                    new_version = current_version.lstrip('v')
                    print(f"ğŸ”§ [{get_relative_path(str(makefile))}] ä¿®æ­£ PKG_VERSION: '{current_version}' -> '{new_version}'")
                    current_content = current_content.replace(current_version_line, f"PKG_VERSION:={new_version}", 1)
                    modified_in_file = True
                    current_version = new_version # Update for release check

                # More complex: Split version-release like 1.2-3 into VERSION=1.2, RELEASE=3
                # This is handled below by the RELEASE check

            # --- Fix PKG_RELEASE ---
            release_match = re.search(r'^(PKG_RELEASE:=)(.*)$', current_content, re.MULTILINE)
            version_present = 'PKG_VERSION:=' in current_content

            new_release_val = None
            if release_match:
                current_release_line = release_match.group(0)
                current_release = release_match.group(2).strip()
                # Must be a positive integer
                if not current_release.isdigit() or int(current_release) <= 0:
                    # Try to extract number if possible, e.g., from "beta1" -> "1"
                    num_part = re.search(r'(\d+)$', current_release)
                    if num_part:
                         new_release_val = num_part.group(1)
                         if int(new_release_val) <= 0: new_release_val = "1" # Ensure positive
                    else:
                         new_release_val = "1" # Default to 1

                    if new_release_val != current_release:
                         print(f"ğŸ”§ [{get_relative_path(str(makefile))}] ä¿®æ­£ PKG_RELEASE: '{current_release}' -> '{new_release_val}'")
                         current_content = current_content.replace(current_release_line, f"PKG_RELEASE:={new_release_val}", 1)
                         modified_in_file = True
            elif version_present:
                # PKG_RELEASE is missing, add it (default to 1)
                # Also handle case where version might be like "1.2.3-5"
                version_match_for_release = re.search(r'^(PKG_VERSION:=)(.*?)(-(\d+))?$', current_content, re.MULTILINE)
                if version_match_for_release:
                    current_version_line = version_match_for_release.group(0)
                    base_version = version_match_for_release.group(2).strip()
                    release_part = version_match_for_release.group(4)

                    if release_part and release_part.isdigit() and int(release_part) > 0:
                        # Version contains release, split it
                        new_version_line = f"PKG_VERSION:={base_version}"
                        new_release_line = f"PKG_RELEASE:={release_part}"
                        print(f"ğŸ”§ [{get_relative_path(str(makefile))}] åˆ†ç¦» PKG_VERSION/RELEASE: '{version_match_for_release.group(2)}{version_match_for_release.group(3) or ''}' -> VERSION='{base_version}', RELEASE='{release_part}'")
                        # Replace version line and insert release line after it
                        current_content = current_content.replace(current_version_line, f"{new_version_line}\n{new_release_line}", 1)
                        modified_in_file = True
                    else:
                        # Version doesn't contain release, just add PKG_RELEASE:=1
                        new_release_line = "PKG_RELEASE:=1"
                        print(f"ğŸ”§ [{get_relative_path(str(makefile))}] æ·»åŠ ç¼ºå¤±çš„ PKG_RELEASE:=1")
                        # Insert after PKG_VERSION line
                        current_content = re.sub(r'^(PKG_VERSION:=.*)$', r'\1\n' + new_release_line, current_content, 1, re.MULTILINE)
                        modified_in_file = True
                else:
                     # Fallback if version format is weird, just add release line
                     new_release_line = "PKG_RELEASE:=1"
                     print(f"ğŸ”§ [{get_relative_path(str(makefile))}] æ·»åŠ ç¼ºå¤±çš„ PKG_RELEASE:=1 (Fallback)")
                     current_content = re.sub(r'^(PKG_VERSION:=.*)$', r'\1\n' + new_release_line, current_content, 1, re.MULTILINE)
                     modified_in_file = True

            # Write back if modified
            if modified_in_file:
                with open(makefile, 'w', encoding='utf-8') as f:
                    f.write(current_content)
                changed_count += 1

        except Exception as e:
            # Ignore errors reading/parsing files that might not be Makefiles
            if isinstance(e, UnicodeDecodeError):
                 pass # Skip binary files etc.
            else:
                 print(f"âš ï¸ å¤„ç†æ–‡ä»¶ {get_relative_path(str(makefile))} æ—¶è·³è¿‡ï¼ŒåŸå› : {e}")
            continue

    print(f"âœ… ä¿®å¤ PKG_VERSION/RELEASE å®Œæˆï¼Œå…±æ£€æŸ¥ {processed_count} ä¸ªæ–‡ä»¶ï¼Œä¿®æ”¹ {changed_count} ä¸ªæ–‡ä»¶ã€‚")
    # Return True if any file was changed, as this might require index update
    return changed_count > 0

def fix_metadata_errors():
    """ä¿®å¤ metadata é”™è¯¯ (åŒ…æ‹¬ç‰ˆæœ¬æ ¼å¼ï¼Œå¹¶æ›´æ–°ç´¢å¼•)"""
    print("ğŸ”§ å°è¯•ä¿®å¤ metadata ç›¸å…³é”™è¯¯...")
    metadata_changed = False

    # 1. Fix PKG_VERSION/RELEASE formats first
    if fix_pkg_version_format():
        metadata_changed = True

    # 2. If formats were fixed or potentially problematic, update feeds index
    if metadata_changed:
        print("â„¹ï¸ æ£€æµ‹åˆ° Makefile æ ¼å¼æ›´æ”¹ï¼Œæ›´æ–° feeds ç´¢å¼•...")
        try:
            update_cmd = ["./scripts/feeds", "update", "-i"]
            print(f"è¿è¡Œ: {' '.join(update_cmd)}")
            result = subprocess.run(update_cmd, check=False, capture_output=True, text=True, encoding='utf-8', errors='replace', timeout=180)
            if result.returncode != 0:
                print(f"âš ï¸ feeds update -i å¤±è´¥:\n{result.stderr[-500:]}")
            else:
                print("âœ… feeds update -i å®Œæˆã€‚")
            # Re-install might be needed if index changed significantly
            install_cmd = ["./scripts/feeds", "install", "-a"]
            print(f"è¿è¡Œ: {' '.join(install_cmd)}")
            result_install = subprocess.run(install_cmd, check=False, capture_output=True, text=True, encoding='utf-8', errors='replace', timeout=300)
            if result_install.returncode != 0:
                 print(f"âš ï¸ feeds install -a å¤±è´¥:\n{result_install.stderr[-500:]}")
            else:
                 print("âœ… feeds install -a å®Œæˆã€‚")

        except subprocess.TimeoutExpired:
            print("âŒ æ‰§è¡Œ feeds update/install æ—¶è¶…æ—¶ã€‚")
            metadata_changed = True # Assume change happened if timeout occurred
        except Exception as e:
            print(f"âŒ æ‰§è¡Œ feeds update/install æ—¶å‡ºé”™: {e}")
            metadata_changed = True # Assume change happened if error occurred

    # 3. Clean tmp directory as a general measure for metadata issues
    tmp_dir = Path("tmp")
    if tmp_dir.exists():
        print("ğŸ§¹ æ¸…ç† tmp ç›®å½•...")
        try:
            shutil.rmtree(tmp_dir)
            print("âœ… tmp ç›®å½•å·²æ¸…ç†ã€‚")
            metadata_changed = True # Cleaning tmp is a change
        except Exception as e:
            print(f"âš ï¸ æ¸…ç† tmp ç›®å½•å¤±è´¥: {e}")

    if metadata_changed:
        print("âœ… Metadata ä¿®å¤å°è¯•å®Œæˆã€‚")
    else:
        print("â„¹ï¸ æœªæ‰§è¡Œ Metadata ç›¸å…³ä¿®å¤ã€‚")

    return metadata_changed

def fix_depends_format(log_content):
    """è‡ªåŠ¨ä¿®å¤ Makefile ä¸­çš„æ— æ•ˆä¾èµ–é¡¹ (å¢å¼ºç‰ˆ v2)"""
    print("ğŸ”§ æ£€æµ‹åˆ°ä¾èµ–é¡¹æ ¼å¼é”™è¯¯ï¼Œå°è¯•è‡ªåŠ¨ä¿®å¤ Makefile ä¸­çš„ DEPENDS å­—æ®µ...")

    reported_files = set()
    # Regex to capture warnings like: WARNING: Makefile 'path/to/Makefile' has a dependency on 'bad-dep>=1.0', which does not exist
    warning_pattern = re.compile(r"WARNING: Makefile '([^']+)' has a dependency on '([^']*)', which does not exist")
    for match in warning_pattern.finditer(log_content):
        # è¿‡æ»¤æ‰ä¸€äº›å·²çŸ¥çš„ã€å¯èƒ½æ— å®³æˆ–éš¾ä»¥ä¿®å¤çš„è­¦å‘Š
        bad_dep = match.group(2).strip()
        makefile_path_str = match.group(1)
        # Filter more aggressively: skip if bad_dep is empty, contains '$', '(', ')', '=>', or known noisy patterns
        if bad_dep and '$' not in bad_dep and '(' not in bad_dep and ')' not in bad_dep and '=>' not in bad_dep \
           and bad_dep.lower() not in ['perl_tests'] and not bad_dep.startswith('gst1-mod-'):
            reported_files.add(makefile_path_str)

    fixed_count = 0
    processed_files = set()
    files_actually_fixed = []

    # ä¼˜å…ˆå¤„ç†æŠ¥å‘Šçš„æ–‡ä»¶
    if reported_files:
        print(f"ğŸ¯ ä¼˜å…ˆå¤„ç†æ—¥å¿—ä¸­æŠ¥å‘Šçš„ {len(reported_files)} ä¸ª Makefile...")
        for makefile_path_str in reported_files:
            makefile_path = Path(makefile_path_str)
            if makefile_path.exists() and makefile_path.is_file():
                resolved_path_str = str(makefile_path.resolve())
                if resolved_path_str not in processed_files:
                    if fix_single_makefile_depends(makefile_path):
                        fixed_count += 1
                        files_actually_fixed.append(get_relative_path(makefile_path_str))
                    processed_files.add(resolved_path_str)
            else:
                print(f"  âš ï¸ æŠ¥å‘Šçš„æ–‡ä»¶ä¸å­˜åœ¨æˆ–ä¸æ˜¯æ–‡ä»¶: {get_relative_path(makefile_path_str)}")

    # --- (ç‰¹å®šé”™è¯¯åŒ…å¤„ç†é€»è¾‘ - å¯é€‰å¢å¼º) ---
    # å¦‚æœ apk_depends_invalid é”™è¯¯å‘ç”Ÿï¼Œä¹Ÿå°è¯•ä¿®å¤é‚£ä¸ªåŒ…çš„ Makefile
    apk_error_sig = get_error_signature(log_content) # Use local log_content here
    if "apk_add_invalid_dep_format" in apk_error_sig: # Check specific error type
        failed_pkg_name = apk_error_sig.split(":")[-1]
        print(f"ğŸ¯ å°è¯•ä¿®å¤å¯¼è‡´ APK é”™è¯¯çš„åŒ… '{failed_pkg_name}' çš„ Makefile...")
        # Search more broadly for the Makefile
        possible_makefile_paths = list(Path(".").glob(f"**/{failed_pkg_name}/Makefile"))
        found_makefile = None
        for mf_path in possible_makefile_paths:
            # Basic check to avoid build_dir etc.
            if not any(ignored in mf_path.parts for ignored in ['build_dir', 'staging_dir', 'tmp', 'dl']):
                found_makefile = mf_path
                break

        if found_makefile:
            resolved_path_str = str(found_makefile.resolve())
            if resolved_path_str not in processed_files:
                print(f"  â¡ï¸ å®šä½åˆ° Makefile: {get_relative_path(str(found_makefile))}")
                if fix_single_makefile_depends(found_makefile):
                    if get_relative_path(str(found_makefile)) not in files_actually_fixed: # é¿å…é‡å¤è®¡æ•°
                         fixed_count += 1
                         files_actually_fixed.append(get_relative_path(str(found_makefile)))
                processed_files.add(resolved_path_str)
            else:
                 print(f"  â„¹ï¸ åŒ… '{failed_pkg_name}' çš„ Makefile å·²å¤„ç†è¿‡ã€‚")
        else:
            print(f"  âš ï¸ æœªèƒ½æ‰¾åˆ°åŒ… '{failed_pkg_name}' çš„ Makefileã€‚")


    if fixed_count > 0:
        print(f"âœ… å…±ä¿®å¤ {fixed_count} ä¸ª Makefile ä¸­çš„ä¾èµ–æ ¼å¼é—®é¢˜: {files_actually_fixed}")
        print("  ğŸ”„ è¿è¡Œ './scripts/feeds update -i && ./scripts/feeds install -a' æ¥æ›´æ–°ä¾èµ–...")
        try:
            update_result = subprocess.run(["./scripts/feeds", "update", "-i"], check=False, capture_output=True, text=True, timeout=180)
            if update_result.returncode != 0: print(f"  âš ï¸ feeds update -i å¤±è´¥:\n{update_result.stderr[-500:]}")
            else: print("    âœ… feeds update -i å®Œæˆã€‚")

            install_result = subprocess.run(["./scripts/feeds", "install", "-a"], check=False, capture_output=True, text=True, timeout=300)
            if install_result.returncode != 0: print(f"  âš ï¸ feeds install -a å¤±è´¥:\n{install_result.stderr[-500:]}")
            else: print("    âœ… feeds install -a å®Œæˆã€‚")
        except subprocess.TimeoutExpired:
             print("  âŒ æ›´æ–°/å®‰è£… feeds æ—¶è¶…æ—¶ã€‚")
        except Exception as e:
            print(f"  âš ï¸ æ›´æ–°/å®‰è£… feeds æ—¶å‡ºé”™: {e}")
        return True
    else:
        print("â„¹ï¸ æœªå‘ç°æˆ–æœªæˆåŠŸä¿®å¤éœ€è¦å¤„ç†çš„ DEPENDS å­—æ®µã€‚")
        return False

def fix_single_makefile_depends(makefile_path: Path):
    """ä¿®å¤å•ä¸ª Makefile ä¸­çš„ DEPENDS å­—æ®µ (å¢å¼ºç‰ˆ v3 - æ›´ç²¾ç¡®æ›¿æ¢)"""
    try:
        with open(makefile_path, 'r', encoding='utf-8', errors='replace') as f:
            content = f.read()
    except Exception as e:
        print(f"  âŒ è¯»å– Makefile å‡ºé”™ {get_relative_path(str(makefile_path))}: {e}")
        return False

    original_content = content
    new_content = content
    modified = False
    offset_adjustment = 0 # Track changes in length for subsequent replacements

    # Find DEPENDS lines (supports += and multi-line with \)
    depends_regex = r'^([ \t]*DEPENDS\s*[:+]?=\s*)((?:.*?\\\n)*.*)$'
    matches = list(re.finditer(depends_regex, content, re.MULTILINE | re.IGNORECASE))

    if not matches:
        return False # No DEPENDS found

    for match in matches:
        start_index = match.start() + offset_adjustment
        end_index = match.end() + offset_adjustment

        original_block = new_content[start_index:end_index] # Get current block from potentially modified content
        prefix = match.group(1)
        depends_str_multiline = match.group(2)

        # Combine multi-line into single line, remove trailing backslashes
        depends_str = depends_str_multiline.replace('\\\n', ' ').replace('\n', ' ').strip()

        # Check for complex Make syntax early
        is_complex = '$' in depends_str or '(' in depends_str

        # Split dependencies by whitespace
        depends_list = re.split(r'\s+', depends_str)
        cleaned_depends = []
        item_modified = False

        for dep in depends_list:
            dep = dep.strip()
            if not dep or dep == '\\': continue

            original_dep = dep
            cleaned_dep = dep # Start with original

            # Remove version constraints only if NOT complex Make syntax
            if not is_complex:
                # Remove prefixes like +@
                dep_prefix = ""
                if dep.startswith('+') or dep.startswith('@'):
                    dep_prefix = dep[0]
                    dep_name = dep[1:]
                else:
                    dep_name = dep

                # Remove version constraints like >=, <=, =, >, <
                dep_name_cleaned = re.split(r'[<>=!~]', dep_name, 1)[0].strip()

                # Basic validation: ensure it looks like a package name
                if dep_name_cleaned and re.match(r'^[a-zA-Z0-9._-]+$', dep_name_cleaned):
                    cleaned_dep = f"{dep_prefix}{dep_name_cleaned}"
                elif dep_name_cleaned: # Looks invalid after cleaning
                    print(f"  âš ï¸ æ¸…ç†åçš„ä¾èµ– '{dep_name_cleaned}' (æ¥è‡ª '{original_dep}') æ ¼å¼æ— æ•ˆï¼Œå·²ä¸¢å¼ƒã€‚æ–‡ä»¶: {get_relative_path(str(makefile_path))}")
                    cleaned_dep = None # Mark for removal
                # else: keep original dep if cleaning results in empty string

            if cleaned_dep is not None: # Add if not marked for removal
                cleaned_depends.append(cleaned_dep)

            if cleaned_dep != original_dep:
                item_modified = True
                print(f"  ğŸ”§ æ¸…ç†ä¾èµ–: '{original_dep}' -> '{cleaned_dep or '(ä¸¢å¼ƒ)'}' in {get_relative_path(str(makefile_path))}")

        if item_modified:
            modified = True # Mark the whole file as modified if any item changed

            # Remove duplicates only for simple lists
            if not is_complex:
                unique_depends = list(dict.fromkeys(cleaned_depends)) # Simple de-duplication
                new_depends_str = ' '.join(unique_depends)
            else:
                new_depends_str = ' '.join(cleaned_depends) # Keep order and duplicates for complex lines

            # Reconstruct the line/block
            # Handle potential multi-line original block - reconstruct as single line for simplicity
            new_depends_line = f"{prefix}{new_depends_str}"

            # Perform replacement in the current state of new_content
            current_block_in_new_content = new_content[start_index:end_index]

            # Check if the block we found is still the same as the original match content
            # This helps avoid incorrect replacement if previous iterations shifted content
            if current_block_in_new_content == original_block:
                new_content = new_content[:start_index] + new_depends_line + new_content[end_index:]
                offset_adjustment += len(new_depends_line) - len(original_block)
            else:
                # Fallback: Try to find the original block text again if content shifted
                # This is less reliable but might work for minor shifts.
                try:
                    current_start_index = new_content.index(original_block, max(0, start_index - 50)) # Search near original position
                    current_end_index = current_start_index + len(original_block)
                    print(f"  âš ï¸ å†…å®¹åç§»ï¼Œå°è¯•åŸºäºåŸå§‹å†…å®¹åœ¨ {current_start_index} å¤„æ›¿æ¢...")
                    new_content = new_content[:current_start_index] + new_depends_line + new_content[current_end_index:]
                    # Recalculate total offset adjustment from the beginning
                    offset_adjustment = len(new_content) - len(original_content)
                except ValueError:
                    print(f"  âŒ æ— æ³•åœ¨å½“å‰å†…å®¹ä¸­é‡æ–°å®šä½åŸå§‹å—ï¼Œè·³è¿‡æ­¤ DEPENDS è¡Œçš„æ›¿æ¢ã€‚æ–‡ä»¶: {get_relative_path(str(makefile_path))}")
                    modified = False # Revert modified status for this block if replacement failed
                    continue # Skip to next match

    if modified:
        try:
            with open(makefile_path, 'w', encoding='utf-8') as f:
                f.write(new_content)
            print(f"  âœ… å·²å†™å›ä¿®æ”¹åˆ°: {get_relative_path(str(makefile_path))}")
            return True
        except Exception as e:
             print(f"  âŒ å†™å› Makefile å¤±è´¥ {get_relative_path(str(makefile_path))}: {e}")
             return False
    else:
        return False # No modification needed or happened

def process_makefile_depends(makefile_path: Path):
    """Helper function to process DEPENDS in a single Makefile.
       Handles simple lists and complex Make constructs differently."""
    try:
        if makefile_path.is_symlink():
            # If it's a symlink, try resolving it, but process the link path if resolution fails
            try:
                real_path = makefile_path.resolve(strict=True)
                if not real_path.is_file(): return False
                makefile_path = real_path
            except Exception:
                # Process the symlink path itself if resolve fails (might be broken link)
                if not makefile_path.exists(): return False # Skip if link target doesn't exist

        if not makefile_path.is_file():
            return False

        with open(makefile_path, 'r', encoding='utf-8', errors='replace') as f:
            content = f.read()
        original_content = content

        # Basic check if it looks like an OpenWrt Makefile
        is_package_makefile = ('define Package/' in content and 'endef' in content) or \
                              ('include $(TOPDIR)/rules.mk' in content or \
                               'include $(INCLUDE_DIR)/package.mk' in content or \
                               'include ../../buildinfo.mk' in content)
        if not is_package_makefile:
            return False

        depends_regex = r'^([ \t]*DEPENDS\s*[:+]?=\s*)((?:.*?\\\n)*.*)$' # Added :? for depends:=
        modified_in_file = False
        new_content = content
        offset_adjustment = 0

        matches = list(re.finditer(depends_regex, content, re.MULTILINE | re.IGNORECASE))
        if not matches:
            return False

        for match in matches:
            start_index = match.start() + offset_adjustment
            end_index = match.end() + offset_adjustment

            original_depends_line_block = new_content[start_index:end_index]
            prefix = match.group(1) # Includes DEPENDS+= or DEPENDS:= etc.
            depends_value = match.group(2).replace('\\\n', ' ').strip()

            # --- Check for complex Make syntax ($ or parenthesis) ---
            is_complex = '$' in depends_value or '(' in depends_value

            # Split by whitespace
            depends_list = re.split(r'\s+', depends_value)
            processed_depends = []
            needs_fix = False

            for dep in depends_list:
                dep = dep.strip()
                if not dep: continue

                original_dep_for_log = dep # Store original for logging comparison
                current_part = dep        # Start with the original part

                # Only clean version constraints if it's NOT complex Make syntax
                if not is_complex:
                    dep_prefix = ""
                    if dep.startswith('+') or dep.startswith('@'):
                        dep_prefix = dep[0]
                        dep_name = dep[1:]
                    else:
                        dep_name = dep

                    # Remove version constraints like >=, <=, =, >, <, ~
                    cleaned_name = re.split(r'[>=<~]', dep_name, 1)[0].strip()

                    # Basic validation: ensure it looks like a package name after cleaning
                    if cleaned_name and re.match(r'^[a-zA-Z0-9._-]+$', cleaned_name):
                        current_part = f"{dep_prefix}{cleaned_name}"
                    elif cleaned_name: # Looks invalid after cleaning
                        print(f"  âš ï¸ æ¸…ç†åçš„ä¾èµ– '{cleaned_name}' (æ¥è‡ª '{original_dep_for_log}') æ ¼å¼æ— æ•ˆï¼Œå·²ä¸¢å¼ƒã€‚æ–‡ä»¶: {get_relative_path(str(makefile_path))}")
                        current_part = None # Mark for removal
                    # else: keep original dep if cleaning results in empty string or no change needed

                if current_part is not None: # Add if not marked for removal
                    processed_depends.append(current_part)

                if current_part != original_dep_for_log:
                    needs_fix = True
                    # Log the change clearly
                    # print(f"  ğŸ”§ æ¸…ç†ä¾èµ–é¡¹éƒ¨åˆ†: '{original_dep_for_log}' -> '{current_part or '(ä¸¢å¼ƒ)'}' in {get_relative_path(str(makefile_path))}")


            # --- Apply fixes only if version constraints were found/removed ---
            if needs_fix:
                if is_complex:
                    # For complex lines, simply join the processed parts back. DO NOT remove duplicates.
                    new_depends_str = ' '.join(processed_depends)
                    # print(f"  å¤„ç†å¤æ‚ä¾èµ–è¡Œ (ä»…ç§»é™¤ç‰ˆæœ¬çº¦æŸ): {get_relative_path(str(makefile_path))}")
                else:
                    # For simple lines, remove duplicates after cleaning.
                    # print(f"  å¤„ç†ç®€å•ä¾èµ–è¡Œ (ç§»é™¤ç‰ˆæœ¬çº¦æŸå’Œé‡å¤é¡¹): {get_relative_path(str(makefile_path))}")
                    # Use dict for ordered unique items
                    seen = {}
                    unique_depends = []
                    for item in processed_depends:
                        item_prefix = ""
                        item_name = item
                        if item.startswith('+') or item.startswith('@'):
                            item_prefix = item[0]
                            item_name = item[1:]

                        if not item_name: continue

                        # Handle + vs @ preference for duplicates
                        if item_name not in seen:
                            seen[item_name] = item_prefix
                            unique_depends.append(item)
                        elif item_prefix == '@' and seen[item_name] == '+':
                            # Upgrade existing '+' to '@'
                            seen[item_name] = '@'
                            # Find and replace in unique_depends list
                            for i, old_item in enumerate(unique_depends):
                                if old_item == f"+{item_name}":
                                    unique_depends[i] = item
                                    break
                        # else: if current is '+' and seen is '@', do nothing (keep '@')
                        # else: if prefixes are same, do nothing (already unique)

                    new_depends_str = ' '.join(unique_depends)

                # Reconstruct the full line/block (usually single line after fix)
                new_depends_line = f"{prefix}{new_depends_str}"

                # Replace the original block within the *current* state of new_content
                current_block_in_new_content = new_content[start_index:end_index]

                if current_block_in_new_content == original_depends_line_block: # Sanity check
                    new_content = new_content[:start_index] + new_depends_line + new_content[end_index:]
                    offset_adjustment += len(new_depends_line) - len(original_depends_line_block)
                    modified_in_file = True
                else:
                     # Fallback: Try to find the original block text again if content shifted
                     try:
                         current_start_index = new_content.index(original_depends_line_block, max(0, start_index - 100)) # Search wider range
                         current_end_index = current_start_index + len(original_depends_line_block)
                         print(f"  âš ï¸ å†…å®¹åç§»ï¼Œå°è¯•åŸºäºåŸå§‹å†…å®¹åœ¨ {current_start_index} å¤„æ›¿æ¢...")
                         new_content = new_content[:current_start_index] + new_depends_line + new_content[current_end_index:]
                         offset_adjustment = len(new_content) - len(original_content) # Recalculate total offset
                         modified_in_file = True
                     except ValueError:
                          print(f"  âŒ æ— æ³•åœ¨å½“å‰å†…å®¹ä¸­é‡æ–°å®šä½åŸå§‹å—ï¼Œè·³è¿‡æ­¤ DEPENDS è¡Œçš„æ›¿æ¢ã€‚æ–‡ä»¶: {get_relative_path(str(makefile_path))}")
                          # Do not set modified_in_file to True if replacement failed
                          continue # Skip to next match

        if modified_in_file:
            print(f"âœ… å·²ä¿®æ”¹ä¾èµ–é¡¹: {get_relative_path(str(makefile_path))}") # Log modified file
            # Write back the modified content only if changes were made
            with open(makefile_path, 'w', encoding='utf-8') as f:
                f.write(new_content)
            return True # Indicate modification

    except Exception as e:
        if isinstance(e, UnicodeDecodeError):
             pass # Skip files that cannot be decoded
        elif isinstance(e, FileNotFoundError):
             print(f"âš ï¸ å¤„ç†æ–‡ä»¶æ—¶æœªæ‰¾åˆ°: {get_relative_path(str(makefile_path))}")
        else:
             # Log other errors during file processing
             print(f"âš ï¸ å¤„ç†æ–‡ä»¶ {get_relative_path(str(makefile_path))} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return False

    return False # No modification needed or happened

def fix_lua_neturl_download(log_content):
    """ä¿®å¤ lua-neturl ä¸‹è½½é—®é¢˜ (éœ€è¦ requests å’Œ beautifulsoup4)"""
    if not (requests and BeautifulSoup):
        print("âŒ è·³è¿‡ lua-neturl ä¸‹è½½ä¿®å¤ï¼šç¼ºå°‘ 'requests' æˆ– 'beautifulsoup4' åº“ã€‚")
        return False

    print("ğŸ”§ æ£€æµ‹åˆ° lua-neturl ä¸‹è½½é”™è¯¯ï¼Œå°è¯•æ›´æ–° Makefile...")

    makefile_path = None
    # Search more broadly
    makefile_paths = list(Path(".").glob("**/lua-neturl/Makefile"))
    if not makefile_paths:
        print("âŒ æ— æ³•æ‰¾åˆ° lua-neturl çš„ Makefileã€‚")
        return False
    # Prioritize paths not in build_dir etc.
    valid_paths = [p for p in makefile_paths if not any(ignored in p.parts for ignored in ['build_dir', 'staging_dir', 'tmp', 'dl'])]
    if not valid_paths:
        print("âŒ æ‰¾åˆ°çš„ lua-neturl Makefile éƒ½åœ¨å¿½ç•¥ç›®å½•ä¸­ã€‚")
        return False
    makefile_path = valid_paths[0] # Take the first valid one

    print(f"æ‰¾åˆ° Makefile: {get_relative_path(str(makefile_path))}")

    try:
        # 1. Get latest tag from GitHub
        print("ğŸŒ æ­£åœ¨ä» GitHub è·å–æœ€æ–°çš„ neturl tag...")
        response = requests.get("https://github.com/golgote/neturl/tags", timeout=20)
        response.raise_for_status() # Raise exception for bad status codes
        soup = BeautifulSoup(response.text, 'html.parser')
        tag_elements = soup.find_all('a', href=re.compile(r'/golgote/neturl/releases/tag/v[\d.-]+'))
        tags = [tag.text.strip() for tag in tag_elements if re.match(r'^v[\d.-]+$', tag.text.strip())]

        if not tags:
            print("âš ï¸ æœªèƒ½åœ¨ GitHub é¡µé¢æ‰¾åˆ°æœ‰æ•ˆçš„ç‰ˆæœ¬æ ‡ç­¾ï¼Œæ— æ³•è‡ªåŠ¨æ›´æ–°ã€‚")
            return False # Cannot proceed without a valid tag
        else:
            # Simple sort might work, but taking the first is often sufficient if newest is first
            latest_tag = tags[0]
            print(f"âœ… è·å–åˆ°æœ€æ–°/ç¬¬ä¸€ä¸ª tag: {latest_tag}")

        # 2. Derive version, source filename, URL, and expected build dir
        raw_version_part = latest_tag.lstrip('v') # e.g., 1.2-1
        pkg_version_match = re.match(r'^(\d+(\.\d+)*)', raw_version_part)
        if not pkg_version_match:
             print(f"âŒ æ— æ³•ä» tag '{latest_tag}' è§£æåŸºç¡€ç‰ˆæœ¬å·ã€‚")
             return False
        pkg_version = pkg_version_match.group(1) # e.g., 1.2

        pkg_release = "1" # Default release
        release_match = re.search(r'-(\d+)$', raw_version_part)
        if release_match:
            pkg_release = release_match.group(1)

        pkg_source_filename = f"neturl-{raw_version_part}.tar.gz" # Use the raw version part for filename
        pkg_source_url = f"https://github.com/golgote/neturl/archive/refs/tags/{latest_tag}.tar.gz"
        expected_build_subdir = f"neturl-{raw_version_part}" # Directory inside tarball

        # 3. Download the source tarball to calculate hash
        dl_dir = Path("./dl")
        dl_dir.mkdir(exist_ok=True)
        tarball_path = dl_dir / pkg_source_filename

        print(f"ä¸‹è½½ {pkg_source_url} åˆ° {get_relative_path(str(tarball_path))}...")
        try:
            # Use wget or curl, whichever is available
            if shutil.which("wget"):
                download_cmd = ["wget", "-q", "-O", str(tarball_path), pkg_source_url]
            elif shutil.which("curl"):
                download_cmd = ["curl", "-s", "-L", "-o", str(tarball_path), pkg_source_url]
            else:
                print("âŒ wget å’Œ curl éƒ½ä¸å¯ç”¨ï¼Œæ— æ³•ä¸‹è½½ã€‚")
                return False
            subprocess.run(download_cmd, check=True, timeout=90)
            print("âœ… ä¸‹è½½æˆåŠŸã€‚")
        except (subprocess.CalledProcessError, subprocess.TimeoutExpired) as e:
            print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
            if tarball_path.exists():
                try: tarball_path.unlink() # Clean up partial download
                except OSError: pass
            return False

        # 4. Calculate SHA256 hash
        sha256_hash = hashlib.sha256()
        with open(tarball_path, "rb") as f:
            while True:
                byte_block = f.read(4096)
                if not byte_block:
                    break
                sha256_hash.update(byte_block)
        sha256_hex = sha256_hash.hexdigest()
        print(f"âœ… è®¡ç®—å¾—åˆ° SHA256 å“ˆå¸Œå€¼: {sha256_hex}")

        # 5. Update the Makefile
        with open(makefile_path, 'r', encoding='utf-8', errors='replace') as f:
            content = f.read()
        original_content = content
        modified_content = content

        # Use functions for safer replacement
        def replace_line(pattern, replacement, text):
            return re.sub(pattern, replacement, text, count=1, flags=re.MULTILINE)

        modified_content = replace_line(r'^(PKG_VERSION:=).*', rf'\g<1>{pkg_version}', modified_content)
        modified_content = replace_line(r'^(PKG_RELEASE:=).*', rf'\g<1>{pkg_release}', modified_content)
        modified_content = replace_line(r'^(PKG_SOURCE:=).*', rf'\g<1>{pkg_source_filename}', modified_content)
        modified_content = replace_line(r'^(PKG_SOURCE_URL:=).*', rf'\g<1>{pkg_source_url}', modified_content)
        modified_content = replace_line(r'^(PKG_HASH:=).*', rf'\g<1>{sha256_hex}', modified_content)

        # Ensure PKG_BUILD_DIR is correct
        build_dir_line = f"PKG_BUILD_DIR:=$(BUILD_DIR)/{expected_build_subdir}"
        build_dir_regex = r'^\s*PKG_BUILD_DIR:=\$\(BUILD_DIR\)/.*'
        if not re.search(build_dir_regex, modified_content, re.MULTILINE):
             insert_after = r'^\s*PKG_HASH:=[^\n]+' # Insert after PKG_HASH
             modified_content = re.sub(f'({insert_after})', f'\\1\n{build_dir_line}', modified_content, 1, re.MULTILINE)
        elif not re.search(r'^\s*PKG_BUILD_DIR:=\$\(BUILD_DIR\)/' + re.escape(expected_build_subdir) + r'\s*$', modified_content, re.MULTILINE):
             modified_content = re.sub(build_dir_regex, build_dir_line, modified_content, 1, re.MULTILINE)

        if modified_content != original_content:
            with open(makefile_path, 'w', encoding='utf-8') as f:
                f.write(modified_content)
            print(f"âœ… Makefile {get_relative_path(str(makefile_path))} å·²æ›´æ–°ã€‚")

            # Clean the package to apply changes
            try:
                pkg_rel_path = makefile_path.parent.relative_to(Path.cwd())
                print(f"ğŸ§¹ æ¸…ç†æ—§çš„æ„å»ºæ–‡ä»¶: {pkg_rel_path}")
                subprocess.run(["make", f"{pkg_rel_path}/clean", "V=s"], check=False, capture_output=True)
            except ValueError:
                 print(f"âš ï¸ æ— æ³•è·å– {makefile_path.parent} çš„ç›¸å¯¹è·¯å¾„è¿›è¡Œæ¸…ç†ã€‚")
            except Exception as e:
                 print(f"âš ï¸ æ‰§è¡Œæ¸…ç†å‘½ä»¤æ—¶å‡ºé”™: {e}")

            print("â³ ç­‰å¾… 2 ç§’åé‡è¯•...")
            time.sleep(2)
            return True
        else:
            print("â„¹ï¸ Makefile æ— éœ€æ›´æ–°ã€‚ä¸‹è½½é—®é¢˜å¯èƒ½ç”±ç½‘ç»œæˆ–å…¶ä»–åŸå› å¼•èµ·ã€‚")
            # Even if Makefile is correct, the download might have failed before.
            # Returning True allows a retry with the potentially fixed download.
            return True

    except requests.exceptions.RequestException as e:
         print(f"âŒ ç½‘ç»œé”™è¯¯: æ— æ³•ä» GitHub è·å–ä¿¡æ¯: {e}")
         return False
    except Exception as e:
        print(f"âŒ æ›´æ–° lua-neturl Makefile æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")
        return False

def fix_apk_directly(makefile_to_fix=None):
    """ç›´æ¥ä¿®å¤ APK ä¾èµ–å‘½ä»¤è¡Œå‚æ•° (ä¿®æ”¹ luci.mk æˆ–æŒ‡å®š Makefile)"""
    target_mk_path_str = ""
    if makefile_to_fix and Path(makefile_to_fix).exists():
        target_mk_path = Path(makefile_to_fix)
        target_mk_path_str = get_relative_path(str(target_mk_path))
        print(f"ğŸ”§ å°è¯•ç›´æ¥ä¿®æ”¹æŒ‡å®šçš„ Makefile '{target_mk_path_str}' æ¥ä¿®å¤ APK ä¾èµ–æ ¼å¼...")
    else:
        print("ğŸ”§ å°è¯•ç›´æ¥ä¿®æ”¹ luci.mk æ¥ä¿®å¤ APK ä¾èµ–æ ¼å¼...")
        luci_mk_path = None
        # ä¼˜å…ˆä½¿ç”¨ feeds ä¸­çš„è·¯å¾„
        possible_paths = ["feeds/luci/luci.mk", "package/feeds/luci/luci.mk", "package/luci/luci.mk"]
        for path in possible_paths:
            if os.path.exists(path):
                luci_mk_path = Path(path)
                break
        if not luci_mk_path:
            print(f"âš ï¸ æ‰¾ä¸åˆ° luci.mk (æ£€æŸ¥è·¯å¾„: {possible_paths})")
            return False
        target_mk_path = luci_mk_path
        target_mk_path_str = get_relative_path(str(target_mk_path))


    try:
        with open(target_mk_path, 'r', encoding='utf-8') as f:
            content = f.read()
        original_content = content

        # æ£€æŸ¥æ˜¯å¦å·²ç»ä¿®å¤è¿‡ (æŸ¥æ‰¾ CleanDependString å®šä¹‰)
        if "define CleanDependString" in content:
            print(f"â„¹ï¸ {target_mk_path_str} ä¼¼ä¹å·²ç»åº”ç”¨è¿‡ä¿®å¤ã€‚")
            return True # è®¤ä¸ºå°è¯•è¿‡æ­¤æ–¹æ³•

        # æ·»åŠ ä¿®å¤ä»£ç ï¼Œä½¿ç”¨ sed æ¥æ¸…ç†ä¾èµ–é¡¹
        fix_code = """

# APK dependency fix v2: Define function to clean dependencies
define CleanDependString
$(strip $(shell echo '$(1)' | tr ' ' '\\n' | sed -e 's/[<>=!~].*//g' -e '/^$$/d' | sort -u | tr '\\n' ' '))
endef

"""
        # æŸ¥æ‰¾æ’å…¥ç‚¹ï¼Œé€šå¸¸åœ¨æ–‡ä»¶é¡¶éƒ¨æˆ– include ä¹‹å
        insert_pos = content.find("include $(TOPDIR)/rules.mk")
        if insert_pos != -1:
            insert_pos = content.find('\n', insert_pos) + 1
            new_content = content[:insert_pos] + fix_code + content[insert_pos:]
        else:
            # Fallback: Insert at the beginning if include is not found
            new_content = fix_code + content

        # ä¿®æ”¹ä¾èµ–å‚æ•°å¤„ç†
        # Match --info "depends:..." part, handle potential variations
        # Use a raw string for the pattern
        original_depends_pattern = r'(--info "depends:)(\$\(PKG_DEPENDS\))(")'
        # Alternative pattern if PKG_DEPENDS is not directly used (less common)
        alternative_depends_pattern = r'(--info "depends:)([^"]+)(")'

        modified_content = new_content
        num_replacements = 0

        # Try replacing the primary pattern first
        modified_content, count1 = re.subn(original_depends_pattern, r'\1$(call CleanDependString,\2)\3', modified_content)
        num_replacements += count1

        # If primary pattern wasn't found, try the alternative (more risky)
        if count1 == 0:
            modified_content, count2 = re.subn(alternative_depends_pattern, r'\1$(call CleanDependString,\2)\3', modified_content)
            num_replacements += count2
            if count2 > 0:
                 print(f"  âš ï¸ ä½¿ç”¨äº†å¤‡ç”¨æ¨¡å¼æ›¿æ¢ä¾èµ–é¡¹ï¼Œè¯·æ£€æŸ¥ {target_mk_path_str} çš„æ­£ç¡®æ€§ã€‚")


        if num_replacements > 0:
            print(f"âœ… å·²åœ¨ {target_mk_path_str} ä¸­æ·»åŠ ä¾èµ–é¡¹æ¸…ç†å‡½æ•°å¹¶ä¿®æ”¹äº† {num_replacements} å¤„ä¾èµ–å‚æ•°ã€‚")
            with open(target_mk_path, 'w', encoding='utf-8') as f:
                f.write(modified_content)

            # Clean tmp directory
            tmp_dir = Path("tmp")
            if tmp_dir.exists():
                print("ğŸ§¹ æ¸…ç† tmp ç›®å½•...")
                try: shutil.rmtree(tmp_dir)
                except Exception as e: print(f"âš ï¸ æ¸…ç† tmp ç›®å½•å¤±è´¥: {e}")

            # Clean potentially affected packages (heuristic)
            print("ğŸ§¹ æ¸…ç†å¯èƒ½å—å½±å“çš„åŒ… (luci-base, toolchain)...")
            subprocess.run(["make", "package/feeds/luci/luci-base/clean", "V=s"], check=False, capture_output=True)
            subprocess.run(["make", "package/libs/toolchain/clean", "V=s"], check=False, capture_output=True)
            # If a specific makefile was targeted, clean that package too
            if makefile_to_fix:
                 try:
                     pkg_rel_path = target_mk_path.parent.relative_to(Path.cwd())
                     print(f"ğŸ§¹ æ¸…ç†ç›®æ ‡åŒ…: {pkg_rel_path}")
                     subprocess.run(["make", f"{pkg_rel_path}/clean", "V=s"], check=False, capture_output=True)
                 except ValueError: pass # Ignore if path is outside CWD
                 except Exception as e: print(f"âš ï¸ æ¸…ç†ç›®æ ‡åŒ…æ—¶å‡ºé”™: {e}")

            return True
        else:
            print(f"âš ï¸ æœªèƒ½åœ¨ {target_mk_path_str} ä¸­æ‰¾åˆ° '--info \"depends:$(PKG_DEPENDS)\"' æˆ–ç±»ä¼¼æ¨¡å¼è¿›è¡Œæ›¿æ¢ã€‚")
            return False # Return False if no modification was made

    except Exception as e:
        print(f"âŒ ç›´æ¥ä¿®å¤ APK ä¾èµ– ({target_mk_path_str}) æ—¶å‡ºé”™: {e}")
        return False

def fix_toolchain_provides_syntax(log_content):
    """ä¿®å¤ toolchain Makefile ä¸­ provides å­—æ®µæœ«å°¾çš„ç©ºæ ¼å¯¼è‡´çš„è¯­æ³•é”™è¯¯"""
    print("ğŸ”§ æ£€æµ‹åˆ° toolchain provides è¯­æ³•é”™è¯¯ï¼Œå°è¯•ä¿®å¤...")
    makefile_path = Path("package/libs/toolchain/Makefile")
    if not makefile_path.exists():
        # Try alternative common location
        makefile_path = Path("toolchain/Makefile")
        if not makefile_path.exists():
            print("âŒ æ‰¾ä¸åˆ° toolchain Makefile (å·²æ£€æŸ¥ package/libs/toolchain/ å’Œ toolchain/)ã€‚")
            return False

    print(f"æ‰¾åˆ° toolchain Makefile: {get_relative_path(str(makefile_path))}")
    fixed = False
    try:
        with open(makefile_path, 'r', encoding='utf-8', errors='replace') as f:
            content = f.read()
        original_content = content

        # Find lines like: --info "provides: name=version " (with trailing space)
        # And remove the trailing space inside the quotes
        # Use a function for replacement to handle multiple occurrences safely
        modified_lines = []
        changed_in_file = False
        for line in content.splitlines():
            original_line = line
            # More specific pattern to avoid accidental replacements
            line = re.sub(r'(--info "provides:)([^"]+?)(\s+)(")', lambda m: f"{m.group(1)}{m.group(2).rstrip()}{m.group(4)}", line)
            if line != original_line:
                changed_in_file = True
            modified_lines.append(line)

        if changed_in_file:
            fixed = True
            new_content = "\n".join(modified_lines)
            with open(makefile_path, 'w', encoding='utf-8') as f:
                f.write(new_content)
            print(f"âœ… å·²ä¿®å¤ {get_relative_path(str(makefile_path))} ä¸­çš„ provides å­—æ®µç©ºæ ¼é—®é¢˜ã€‚")
            # Clean toolchain package
            print("ğŸ§¹ æ¸…ç† toolchain æ„å»º...")
            # Determine make target path based on found Makefile location
            if makefile_path.parts[0] == 'toolchain':
                 clean_target = "toolchain/clean"
            else:
                 clean_target = "package/libs/toolchain/clean"
            subprocess.run(["make", clean_target, "V=s"], check=False, capture_output=True)
            return True
        else:
            print("â„¹ï¸ æœªåœ¨ toolchain Makefile ä¸­æ‰¾åˆ°éœ€è¦ä¿®å¤çš„ provides å­—æ®µç©ºæ ¼ã€‚")
            return False

    except Exception as e:
        print(f"âŒ ä¿®å¤ toolchain provides è¯­æ³•æ—¶å‡ºé”™: {e}")
        return False

def fix_apk_wrapper_issues(log_content):
    """å¤„ç†ä¸ apk wrapper ç›¸å…³çš„é—®é¢˜ (ç§»é™¤æˆ–ä¿®å¤)"""
    wrapper_path = Path("staging_dir/host/bin/apk")
    real_path = Path("staging_dir/host/bin/apk.real")

    if real_path.exists(): # Wrapper exists (or did exist)
        print("ğŸ”§ æ£€æµ‹åˆ° apk wrapper æˆ–å…¶æ®‹ç•™ï¼Œè¿›è¡Œå¤„ç†...")
        if wrapper_path.exists():
             # Check if it's our wrapper causing syntax errors
             syntax_error_in_log = "Syntax error:" in log_content and str(wrapper_path) in log_content
             if syntax_error_in_log:
                  print("âš ï¸ æ£€æµ‹åˆ° wrapper è„šæœ¬å­˜åœ¨è¯­æ³•é”™è¯¯ï¼Œç§»é™¤ wrapper å¹¶æ¢å¤åŸå§‹ apk...")
                  try:
                       wrapper_path.unlink()
                       real_path.rename(wrapper_path)
                       wrapper_path.chmod(0o755) # Restore permissions
                       print("âœ… å·²æ¢å¤åŸå§‹ apk å‘½ä»¤ã€‚")
                       return True # Action taken
                  except Exception as e:
                       print(f"âŒ æ¢å¤åŸå§‹ apk æ—¶å‡ºé”™: {e}")
                       # Try deleting the wrapper anyway if rename failed
                       try: wrapper_path.unlink()
                       except OSError: pass
                       return False
             else:
                  print("â„¹ï¸ wrapper å­˜åœ¨ä½†æ—¥å¿—ä¸­æœªæ£€æµ‹åˆ°å…¶è¯­æ³•é”™è¯¯ã€‚å¯èƒ½ä¸éœ€è¦å¤„ç†ã€‚")
                  return False # No action taken on the wrapper itself
        else:
             # Wrapper script is missing, but real binary exists. Restore.
             print("âš ï¸ wrapper è„šæœ¬ä¸¢å¤±ï¼Œä½†å¤‡ä»½å­˜åœ¨ã€‚æ¢å¤åŸå§‹ apk...")
             try:
                  real_path.rename(wrapper_path)
                  wrapper_path.chmod(0o755)
                  print("âœ… å·²æ¢å¤åŸå§‹ apk å‘½ä»¤ã€‚")
                  return True # Action taken
             except Exception as e:
                  print(f"âŒ æ¢å¤åŸå§‹ apk æ—¶å‡ºé”™: {e}")
                  return False
    else:
         # No wrapper seems to be active
         # Check if the current apk is a script (might be an old broken wrapper without .real)
         if wrapper_path.is_file() and not wrapper_path.is_symlink():
             try:
                 with open(wrapper_path, 'r') as f:
                     first_line = f.readline()
                 if first_line.startswith("#!"): # It's a script!
                      print(f"âš ï¸ {wrapper_path} æ˜¯ä¸€ä¸ªè„šæœ¬ä½†æ²¡æœ‰ .real å¤‡ä»½ã€‚å¯èƒ½æ˜¯æŸåçš„ wrapperã€‚å°è¯•åˆ é™¤...")
                      try:
                           wrapper_path.unlink()
                           print(f"âœ… å·²åˆ é™¤å¯èƒ½æ˜¯ wrapper çš„è„šæœ¬: {get_relative_path(str(wrapper_path))}")
                           print("   ä¸‹ä¸€æ­¥ç¼–è¯‘å¯èƒ½ä¼šå› ç¼ºå°‘ apk è€Œå¤±è´¥ï¼Œä½†æ¸…é™¤äº†æ½œåœ¨é—®é¢˜ã€‚")
                           return True # Action taken (deletion)
                      except Exception as e:
                           print(f"âŒ åˆ é™¤è„šæœ¬ {get_relative_path(str(wrapper_path))} å¤±è´¥: {e}")
                           return False
             except Exception:
                 pass # Ignore errors reading the file

         # If it's not a script or doesn't exist, no wrapper issue detected
         return False # No action taken

def fix_apk_depends_problem():
    """
    ç»¼åˆå¤„ç† APK ä¾èµ–æ ¼å¼é”™è¯¯ (Error 99 æˆ– invalid value)ã€‚
    ä¼˜å…ˆå°è¯•ä¿®å¤ç‰¹å®šåŒ… Makefile é—®é¢˜ (å¦‚ luci-lib-taskd)ï¼Œç„¶åå°è¯•ä¿®æ”¹ luci.mkã€‚
    """
    print("ğŸ” å°è¯•ç»¼åˆè§£å†³æ–¹æ¡ˆä¿®å¤ APK ä¾èµ–æ ¼å¼é—®é¢˜...")
    fixed_something = False

    # æ­¥éª¤ 1: ä¸“é—¨ä¿®å¤ luci-lib-taskd çš„ LUCI_EXTRA_DEPENDS (High priority if applicable)
    print("  æ–¹æ³• 1: å°è¯•æ³¨é‡Šæ‰ luci-lib-taskd/Makefile ä¸­çš„ LUCI_EXTRA_DEPENDS...")
    if fix_luci_lib_taskd_extra_depends():
        print("  âœ… æ–¹æ³• 1 (æ³¨é‡Š LUCI_EXTRA_DEPENDS) æ‰§è¡Œå®Œæˆã€‚")
        fixed_something = True
    else:
        print("  â„¹ï¸ æ–¹æ³• 1 (æ³¨é‡Š LUCI_EXTRA_DEPENDS) æœªè¿›è¡Œä¿®æ”¹æˆ–å¤±è´¥ã€‚")

    # æ­¥éª¤ 2: å¦‚æœä¸Šä¸€æ­¥æ— æ•ˆæˆ–ä¸é€‚ç”¨ï¼Œå†å°è¯•ä¿®æ”¹ luci.mk (ä½œä¸ºé€šç”¨ä¿®å¤)
    if not fixed_something:
        print("  æ–¹æ³• 2: å°è¯•ç›´æ¥ä¿®æ”¹ luci.mk ä¸­çš„ apk mkpkg è°ƒç”¨...")
        if fix_apk_directly(): # Pass no specific file, targets luci.mk
            print("  âœ… æ–¹æ³• 2 (ä¿®æ”¹ luci.mk) æ‰§è¡Œå®Œæˆã€‚")
            fixed_something = True
        else:
            print("  â„¹ï¸ æ–¹æ³• 2 (ä¿®æ”¹ luci.mk) æœªè¿›è¡Œä¿®æ”¹æˆ–å¤±è´¥ã€‚")

    # æ­¥éª¤ 3: (å¯é€‰) å°è¯•ä¿®å¤å…·ä½“å¯¼è‡´é”™è¯¯çš„åŒ…çš„ DEPENDS:= è¡Œ
    # This might be redundant if luci.mk fix works globally, but can target specific issues.
    if not fixed_something:
        apk_error_sig = get_error_signature(log_content_global) # Use global log content
        if "apk_add_invalid_dep_format" in apk_error_sig:
            failed_pkg_name = apk_error_sig.split(":")[-1]
            if failed_pkg_name != "unknown_pkg_from_apk":
                print(f"  æ–¹æ³• 3: å°è¯•ä¿®å¤åŒ… '{failed_pkg_name}' çš„ Makefile DEPENDS...")
                possible_makefile_paths = list(Path(".").glob(f"**/{failed_pkg_name}/Makefile"))
                found_makefile = None
                for mf_path in possible_makefile_paths:
                    if not any(ignored in mf_path.parts for ignored in ['build_dir', 'staging_dir', 'tmp', 'dl']):
                        found_makefile = mf_path
                        break
                if found_makefile:
                     if fix_single_makefile_depends(found_makefile):
                          print(f"  âœ… æ–¹æ³• 3 (ä¿®å¤ {failed_pkg_name} DEPENDS) æ‰§è¡Œå®Œæˆã€‚")
                          fixed_something = True
                     else:
                          print(f"  â„¹ï¸ æ–¹æ³• 3 (ä¿®å¤ {failed_pkg_name} DEPENDS) æœªè¿›è¡Œä¿®æ”¹æˆ–å¤±è´¥ã€‚")
                else:
                     print(f"  âš ï¸ æ–¹æ³• 3: æœªæ‰¾åˆ°åŒ… '{failed_pkg_name}' çš„ Makefileã€‚")

    return fixed_something

def fix_luci_lib_taskd_extra_depends():
    """ä¸“é—¨æ³¨é‡Šæ‰ luci-lib-taskd/Makefile ä¸­çš„ LUCI_EXTRA_DEPENDS è¡Œ"""
    print("ğŸ”§ å°è¯•ç‰¹å®šä¿®å¤: æ³¨é‡Šæ‰ luci-lib-taskd/Makefile ä¸­çš„ LUCI_EXTRA_DEPENDS...")
    makefile_path = None
    # ç²¾ç¡®æŸ¥æ‰¾ Makefile
    possible_paths = list(Path(".").glob("**/luci-lib-taskd/Makefile"))
    if not possible_paths:
        print(f"  âš ï¸ æœªæ‰¾åˆ° luci-lib-taskd çš„ Makefileã€‚")
        return False

    # Filter out paths in ignored directories
    valid_paths = [p for p in possible_paths if not any(ignored in p.parts for ignored in ['build_dir', 'staging_dir', 'tmp', 'dl'])]
    if not valid_paths:
        print(f"  âš ï¸ æ‰¾åˆ°çš„ luci-lib-taskd Makefile éƒ½åœ¨å¿½ç•¥ç›®å½•ä¸­ã€‚")
        return False
    makefile_path = valid_paths[0] # Take the first valid one
    makefile_path_rel = get_relative_path(str(makefile_path))
    print(f"  â¡ï¸ å®šä½åˆ° Makefile: {makefile_path_rel}")

    try:
        with open(makefile_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()

        new_lines = []
        modified = False
        found_target_line = False

        # ç²¾ç¡®åŒ¹é…éœ€è¦æ³¨é‡Šæ‰çš„è¡Œ (allow variations in spacing and version)
        target_line_pattern = re.compile(r"^\s*LUCI_EXTRA_DEPENDS\s*[:+]?=\s*\+?taskd\s*(?:\(.*\))?\s*$", re.IGNORECASE)

        for i, line in enumerate(lines):
            stripped_line = line.strip()
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç›®æ ‡è¡Œä¸”æœªè¢«æ³¨é‡Š
            if target_line_pattern.match(stripped_line) and not stripped_line.startswith("#"):
                found_target_line = True
                print(f"  ğŸ”§ åœ¨è¡Œ {i+1} æ³¨é‡Šæ‰: {line.strip()}")
                new_lines.append("#" + line) # åœ¨è¡Œé¦–æ·»åŠ  #
                modified = True
            # æ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯è¢«æ³¨é‡Šçš„ç›®æ ‡è¡Œ
            elif stripped_line.startswith("#") and target_line_pattern.match(stripped_line.lstrip("#").strip()):
                 found_target_line = True
                 print(f"  â„¹ï¸ åœ¨è¡Œ {i+1} å‘ç°å·²æ³¨é‡Šçš„ç›®æ ‡è¡Œ: {line.strip()}")
                 new_lines.append(line) # ä¿æŒæ³¨é‡ŠçŠ¶æ€
            else:
                new_lines.append(line)

        if not found_target_line:
             print(f"  âš ï¸ æœªæ‰¾åˆ°éœ€è¦æ³¨é‡Šçš„ LUCI_EXTRA_DEPENDS è¡Œã€‚")
             # Check DEPENDS as a fallback indicator of manual fix
             define_block_pattern = re.compile(r'define Package/luci-lib-taskd\s*.*?\s*DEPENDS\s*:=\s*.*?\+taskd\s+', re.DOTALL | re.IGNORECASE)
             if define_block_pattern.search("".join(lines)):
                 print("  â„¹ï¸ æ£€æµ‹åˆ° DEPENDS å¯èƒ½å·²è¢«æ‰‹åŠ¨ä¿®å¤ã€‚")
                 return True # Assume problem is addressed
             return False # Truly not found

        if modified:
            print(f"  âœ… å‡†å¤‡å†™å›ä¿®æ”¹åˆ° {makefile_path_rel}")
            with open(makefile_path, 'w', encoding='utf-8') as f:
                f.writelines(new_lines)
            # æ¸…ç†è¯¥åŒ…çš„ç¼“å­˜
            try:
                pkg_rel_path = makefile_path.parent.relative_to(Path.cwd())
                print(f"  ğŸ§¹ æ¸…ç†åŒ… '{pkg_rel_path}' ç¼“å­˜ (DIRCLEAN)...")
                subprocess.run(["make", f"DIRCLEAN=1", f"{pkg_rel_path}/clean", "V=s"], check=False, capture_output=True)
            except ValueError:
                 print(f"  âš ï¸ æ— æ³•è·å– {makefile_path.parent} çš„ç›¸å¯¹è·¯å¾„è¿›è¡Œæ¸…ç†ã€‚")
            except Exception as e:
                 print(f"  âš ï¸ æ‰§è¡Œæ¸…ç†å‘½ä»¤æ—¶å‡ºé”™: {e}")

            # Clean tmp directory as well
            tmp_dir = Path("tmp")
            if tmp_dir.exists():
                print("  ğŸ§¹ æ¸…ç† tmp ç›®å½•...")
                try: shutil.rmtree(tmp_dir); print("    âœ… tmp ç›®å½•å·²åˆ é™¤ã€‚")
                except Exception as e: print(f"    âš ï¸ æ¸…ç† tmp ç›®å½•å¤±è´¥: {e}")
            return True
        else:
            print(f"  â„¹ï¸ {makefile_path_rel} æ— éœ€ä¿®æ”¹ (LUCI_EXTRA_DEPENDS å·²æ³¨é‡Šæˆ–ä¸å­˜åœ¨)ã€‚")
            return True # Assume problem is addressed or not applicable

    except Exception as e:
        print(f"âŒ ä¿®æ”¹åŒ… 'luci-lib-taskd' çš„ Makefile æ—¶å‡ºé”™: {e}")
        return False

def fix_apk_wrapper_syntax():
    """ä¿®å¤ APK åŒ…è£…å™¨è„šæœ¬ä¸­çš„è¯­æ³•é”™è¯¯"""
    print("ğŸ”§ æ£€æµ‹åˆ° APK wrapper è¯­æ³•é”™è¯¯ï¼Œå°è¯•ä¿®å¤...")

    wrapper_path = Path("staging_dir/host/bin/apk")
    real_path = Path("staging_dir/host/bin/apk.real")
    wrapper_path_rel = get_relative_path(str(wrapper_path))
    real_path_rel = get_relative_path(str(real_path))

    if wrapper_path.exists() and real_path.exists():
        try:
            # è¯»å–å½“å‰çš„åŒ…è£…å™¨è„šæœ¬
            with open(wrapper_path, 'r') as f:
                content = f.read()

            # æ£€æŸ¥æ˜¯å¦æ˜¯æˆ‘ä»¬çš„ wrapper (é€šè¿‡æ³¨é‡Šæˆ–ç‰¹å¾åˆ¤æ–­)
            if "# APK wrapper script" in content or 'REAL_APK=' in content:
                print(f"  â„¹ï¸ æ£€æµ‹åˆ°æ—§çš„/é”™è¯¯çš„ APK wrapper ({wrapper_path_rel})ï¼Œç§»é™¤å¹¶æ¢å¤åŸå§‹å‘½ä»¤...")
                wrapper_path.unlink() # åˆ é™¤è„šæœ¬
                real_path.rename(wrapper_path) # æ¢å¤åŸå§‹å‘½ä»¤
                wrapper_path.chmod(0o755) # æ¢å¤æƒé™
                print(f"  âœ… å·²æ¢å¤åŸå§‹ APK å‘½ä»¤ ({wrapper_path_rel})ã€‚")

                # æ¢å¤åï¼Œå°è¯•ç›´æ¥ä¿®å¤ä¾èµ–é—®é¢˜ï¼Œå› ä¸ºè¿™å¯èƒ½æ˜¯æ ¹æœ¬åŸå› 
                print("  â–¶ï¸ å°è¯•å†æ¬¡è¿è¡Œç›´æ¥ä¿®å¤ (luci.mk)...")
                return fix_apk_directly() # è¿”å›ç›´æ¥ä¿®å¤çš„ç»“æœ
            else:
                print(f"  âš ï¸ {wrapper_path_rel} å­˜åœ¨ä½†ä¸æ˜¯é¢„æœŸçš„ wrapper è„šæœ¬ã€‚")
                # å¯èƒ½æ˜¯å…¶ä»–ä¸œè¥¿ï¼Œä¸è¦åŠ¨å®ƒï¼Œè¿”å› False
                return False
        except Exception as e:
            print(f"âŒ ç§»é™¤æ—§ wrapper æˆ–æ¢å¤åŸå§‹ apk æ—¶å‡ºé”™: {e}")
            return False
    elif wrapper_path.exists() and not real_path.exists():
         print(f"  âš ï¸ æ‰¾åˆ° {wrapper_path_rel} ä½†æ²¡æœ‰å¤‡ä»½ {real_path_rel}ã€‚å¯èƒ½æ˜¯åŸå§‹ apk æˆ–æŸåçš„ wrapperã€‚")
         # å°è¯•æ£€æŸ¥å®ƒæ˜¯å¦æ˜¯è„šæœ¬
         is_script = False
         try:
             with open(wrapper_path, 'r') as f:
                 first_line = f.readline()
             if first_line.startswith("#!"): is_script = True
         except Exception: pass

         if is_script:
             print(f"  âš ï¸ {wrapper_path_rel} æ˜¯ä¸€ä¸ªè„šæœ¬ï¼Œå¯èƒ½æ˜¯æŸåçš„ wrapperã€‚å°è¯•åˆ é™¤...")
             try:
                 wrapper_path.unlink()
                 print(f"  âœ… å·²åˆ é™¤è„šæœ¬: {wrapper_path_rel}")
                 return True # Action taken
             except Exception as e:
                 print(f"  âŒ åˆ é™¤è„šæœ¬å¤±è´¥: {e}")
                 return False
         else:
             # Assume it's the original apk, try direct fix
             print("  â–¶ï¸ å‡è®¾æ˜¯åŸå§‹ APKï¼Œå°è¯•è¿è¡Œç›´æ¥ä¿®å¤ (luci.mk)...")
             return fix_apk_directly()
    else:
        print(f"  âš ï¸ æ‰¾ä¸åˆ° APK wrapper ({wrapper_path_rel}) æˆ–åŸå§‹å¤‡ä»½ ({real_path_rel})ã€‚")
        # å°è¯•ç›´æ¥ä¿®å¤
        print("  â–¶ï¸ å°è¯•è¿è¡Œç›´æ¥ä¿®å¤ (luci.mk)...")
        return fix_apk_directly()

def fix_apk_add_base_files_issue(log_content):
    """ä¿®å¤ apk add æ—¶ base-files= æˆ–ç±»ä¼¼åŒ…ç‰ˆæœ¬ç¼ºå¤±å¯¼è‡´çš„ Error 99 (v11: è®¾ç½®é¢„å¤„ç†æ ‡å¿—)"""
    global needs_base_files_precompute
    print("ğŸ”§ æ£€æµ‹åˆ° apk add æ— æ•ˆä¾èµ–æ ¼å¼é”™è¯¯ (é€šå¸¸ç”± base-files ç‰ˆæœ¬ç¼ºå¤±å¼•èµ·)ã€‚")
    print(f"  è®¾ç½®æ ‡å¿—ï¼Œåœ¨ä¸‹æ¬¡å°è¯•å‰é¢„å…ˆç¼–è¯‘ base-files å¹¶ä¿®å¤ç‰ˆæœ¬æ–‡ä»¶å...")

    action_taken = False

    # --- Perform minimal cleanup ---
    tmp_dir = Path("tmp")
    if tmp_dir.exists():
        print(f"  ğŸ§¹ æ¸…ç†ç›®å½•: {get_relative_path(str(tmp_dir))}")
        try:
            shutil.rmtree(tmp_dir)
            action_taken = True
        except Exception as e:
            print(f"    âš ï¸ æ¸…ç† {tmp_dir} ç›®å½•å¤±è´¥: {e}")
            action_taken = True # Still counts as an attempt
    # Ensure tmp exists for subsequent steps
    try:
        tmp_dir.mkdir(exist_ok=True)
    except Exception as e:
        print(f"    âš ï¸ åˆ›å»º {tmp_dir} ç›®å½•å¤±è´¥: {e}")

    # Clean staging package directory (more specific target)
    staging_pkg_dir_path = None
    # Try to find the specific target staging dir mentioned in logs if possible
    target_staging_match = re.search(r'staging_dir/target-([a-zA-Z0-9_.-]+)', log_content)
    if target_staging_match:
        target_name = target_staging_match.group(1)
        # Construct path like staging_dir/target-mipsel_24kc_musl/pkginfo
        pkginfo_dir = Path("staging_dir") / f"target-{target_name}" / "pkginfo"
        if pkginfo_dir.exists():
             # Clean the pkginfo dir as it contains dependency info
             staging_pkg_dir_path = pkginfo_dir # Target this dir for cleaning
        else:
             # Fallback to cleaning the packages dir for the arch
             arch_match = re.search(r'mipsel|aarch64|x86_64|arm', target_name) # Basic arch detection
             if arch_match:
                  arch = arch_match.group(0)
                  # Heuristic: try common package dir names
                  for pkg_dir_name in [arch, f"{arch}_core", f"{arch}_generic", "all"]:
                       potential_path = Path("staging_dir/packages") / pkg_dir_name
                       if potential_path.exists():
                           staging_pkg_dir_path = potential_path
                           break
    # Fallback if no specific dir found
    if not staging_pkg_dir_path:
         staging_pkg_dir_path = Path("staging_dir/packages") # Clean the whole packages dir as last resort

    if staging_pkg_dir_path and staging_pkg_dir_path.exists():
        print(f"  ğŸ§¹ æ¸…ç†ç›®å½•: {get_relative_path(str(staging_pkg_dir_path))}")
        try:
            # Be careful cleaning staging_dir/packages directly, maybe just clean specific arch?
            # For now, let's stick to cleaning the determined path
            if staging_pkg_dir_path.name == "packages" and staging_pkg_dir_path.parent.name == "staging_dir":
                print("    âš ï¸ è­¦å‘Š: å°†æ¸…ç†æ•´ä¸ª staging_dir/packages ç›®å½•ã€‚")
            shutil.rmtree(staging_pkg_dir_path)
            action_taken = True
        except Exception as e:
            print(f"    âš ï¸ æ¸…ç† {get_relative_path(str(staging_pkg_dir_path))} ç›®å½•å¤±è´¥: {e}")
            action_taken = True

    # --- Set the flag ---
    needs_base_files_precompute = True
    print("  âœ… å·²è®¾ç½® base-files é¢„å¤„ç†æ ‡å¿—ã€‚")

    # Return True to indicate a fix strategy was determined
    return True
# --- Main Logic ---
def main():
    parser = argparse.ArgumentParser(description='OpenWrt ç¼–è¯‘ä¿®å¤è„šæœ¬')
    parser.add_argument('make_command', help='åŸå§‹ç¼–è¯‘å‘½ä»¤ï¼Œä¾‹å¦‚ "make V=s"')
    parser.add_argument('log_file', help='ä¸»æ—¥å¿—æ–‡ä»¶åŸºç¡€å (ä¸å« .run.N.log)')
    parser.add_argument('--max-retry', type=int, default=8, help='æœ€å¤§é‡è¯•æ¬¡æ•°')
    parser.add_argument('--jobs', type=int, default=0, help='åˆå§‹å¹¶è¡Œä»»åŠ¡æ•° (0 è¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹)')
    args = parser.parse_args()

    # Extract base command without -j flag
    base_cmd = re.sub(r'\s-j\s*\d+', '', args.make_command).strip()
    # Determine initial jobs
    jobs = args.jobs if args.jobs > 0 else (os.cpu_count() or 1)
    print(f"åˆå§‹å¹¶è¡Œä»»åŠ¡æ•°: {jobs}")

    retry = 1
    last_error_signature = None
    same_error_count = 0
    global log_content_global # Allow modification
    global needs_base_files_precompute # Allow modification

    while retry <= args.max_retry:
        # --- Pre-computation Step (if flagged) ---
        if needs_base_files_precompute:
            print(f"\nğŸš€ [å°è¯• {retry-1} å] æ‰§è¡Œé¢„å¤„ç†æ­¥éª¤ï¼šç¼–è¯‘ base-files...")
            precompute_cmd = f"{base_cmd} package/base-files/compile V=s -j1" # Compile base-files specifically
            print(f"è¿è¡Œ: {precompute_cmd}")
            pre_log_file = f"{args.log_file}.pre.{retry}.log"
            pre_status = -1
            try:
                with open(pre_log_file, 'w', encoding='utf-8', errors='replace') as plog:
                    process = subprocess.run(precompute_cmd, shell=True, stdout=plog, stderr=subprocess.STDOUT, timeout=300) # Add timeout
                    pre_status = process.returncode
            except subprocess.TimeoutExpired:
                 print(f"âŒ base-files é¢„ç¼–è¯‘è¶…æ—¶ (æ—¥å¿—: {pre_log_file})")
            except Exception as e:
                 print(f"âŒ base-files é¢„ç¼–è¯‘æ—¶å‘ç”Ÿé”™è¯¯: {e} (æ—¥å¿—: {pre_log_file})")


            if pre_status == 0:
                print("âœ… base-files é¢„ç¼–è¯‘æˆåŠŸã€‚")
                # Find and rename the apk file if necessary
                try:
                    # Find staging package dir more reliably
                    staging_pkg_dir = None
                    for p in Path("staging_dir/packages").iterdir():
                        if p.is_dir(): # Assume the first directory found is the target arch
                            staging_pkg_dir = p
                            break
                    if staging_pkg_dir:
                        base_files_apks = list(staging_pkg_dir.glob("base-files_*.apk"))
                        for apk_path in base_files_apks:
                            if "=" not in apk_path.name:
                                # Extract version like 2023-01-01-abcdef12 or just a number
                                version_match = re.search(r'_([\d.-]+(?:_[a-f0-9]+)?(?:-r\d+)?)_', apk_path.name)
                                if version_match:
                                    version = version_match.group(1)
                                    new_name = f"base-files={version}.apk"
                                    new_path = apk_path.with_name(new_name)
                                    print(f"  ğŸ·ï¸ é‡å‘½å base-files APK: {apk_path.name} -> {new_path.name}")
                                    try:
                                        apk_path.rename(new_path)
                                    except OSError as rename_e:
                                         print(f"    âš ï¸ é‡å‘½åå¤±è´¥: {rename_e}")
                                else:
                                     print(f"  âš ï¸ æ— æ³•ä» {apk_path.name} æå–ç‰ˆæœ¬ä»¥é‡å‘½åã€‚")
                    else:
                        print("  âš ï¸ æœªæ‰¾åˆ° staging_dir/packages/<arch> ç›®å½•æ¥æ£€æŸ¥ base-files APKã€‚")
                except Exception as e:
                    print(f"  âš ï¸ é‡å‘½å base-files APK æ—¶å‡ºé”™: {e}")
            elif pre_status != -1: # If not timeout/exception
                print(f"âŒ base-files é¢„ç¼–è¯‘å¤±è´¥ï¼Œè¿”å›ç : {pre_status} (æ—¥å¿—: {pre_log_file})ï¼Œç»§ç»­å°è¯•ä¸»ç¼–è¯‘...")
            needs_base_files_precompute = False # Reset flag regardless of outcome

        # --- Main Compile Step ---
        current_run_log = f"{args.log_file}.run.{retry}.log"
        cmd = f"{base_cmd} -j{jobs}"
        print(f"\n--- å°è¯• {retry}/{args.max_retry} ---")
        print(f"è¿è¡Œå‘½ä»¤: {cmd}")
        print(f"æ—¥å¿—æ–‡ä»¶: {current_run_log}")

        status = -1 # Default status
        try:
            # Use Popen to stream output and write to log simultaneously
            process = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,
                                       text=True, encoding='utf-8', errors='replace', bufsize=1) # Line buffered

            with open(current_run_log, 'w', encoding='utf-8', errors='replace') as f:
                for line in iter(process.stdout.readline, ''):
                    sys.stdout.write(line)
                    f.write(line)
            status = process.wait() # Get final return code

        except Exception as e:
            print(f"\nâŒ æ‰§è¡Œç¼–è¯‘å‘½ä»¤æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
            # Write exception to log if possible
            try:
                with open(current_run_log, 'a', encoding='utf-8', errors='replace') as f:
                    f.write(f"\n\n*** SCRIPT ERROR DURING EXECUTION ***\n{e}\n")
            except Exception: pass
            status = 1 # Assume failure

        # --- Process Results ---
        if status == 0:
            print("\nâœ… ç¼–è¯‘æˆåŠŸï¼")
            return 0

        print(f"\nâŒ ç¼–è¯‘å¤±è´¥ (è¿”å›ç : {status})")

        # Read log content for error analysis
        try:
            with open(current_run_log, 'r', encoding='utf-8', errors='replace') as f:
                log_content_global = f.read()
        except FileNotFoundError:
             print(f"âŒ æ— æ³•è¯»å–æ—¥å¿—æ–‡ä»¶: {current_run_log}")
             log_content_global = "" # Reset log content
             current_error_signature = "no_log_content_error"
        except Exception as e:
             print(f"âŒ è¯»å–æ—¥å¿—æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
             log_content_global = ""
             current_error_signature = "log_read_error"
        else:
             current_error_signature = get_error_signature(log_content_global)

        print(f"æ£€æµ‹åˆ°çš„é”™è¯¯ç­¾å: {current_error_signature}")

        # --- Consecutive Error Check ---
        if current_error_signature == last_error_signature and current_error_signature not in ["no_log_content", "unknown_error", "log_read_error"]:
            same_error_count += 1
            print(f"è¿ç»­ç›¸åŒé”™è¯¯æ¬¡æ•°: {same_error_count + 1}")
            # Define thresholds for stopping
            # More tolerant for dependency/metadata issues which might need multiple steps
            if current_error_signature.startswith(("apk_", "makefile_dep_", "metadata_")):
                consecutive_threshold = 3
            else:
                consecutive_threshold = 2

            if same_error_count >= consecutive_threshold:
                print(f"é”™è¯¯ '{current_error_signature}' è¿ç»­å‡ºç° {same_error_count + 1} æ¬¡ï¼Œè¾¾åˆ°é˜ˆå€¼ {consecutive_threshold+1}ï¼Œåœæ­¢é‡è¯•ã€‚")
                break # Exit the while loop
        else:
            same_error_count = 0 # Reset counter if error changes

        last_error_signature = current_error_signature

        # --- Attempt Fixes ---
        fix_attempted = False
        if current_error_signature == "oom_detected":
            new_jobs = handle_oom(jobs, log_content_global)
            if new_jobs != jobs:
                jobs = new_jobs
                fix_attempted = True
        elif current_error_signature.startswith("netifd_link_error"):
            fix_attempted = fix_netifd_libnl_tiny()
        elif current_error_signature == "lua_neturl_download":
            fix_attempted = fix_lua_neturl_download(log_content_global)
        elif current_error_signature.startswith("apk_invalid_version_format:"):
            fix_attempted = fix_metadata_errors() # This handles version format issues
        elif current_error_signature == "trojan_plus_build_error": # Renamed signature
            fix_attempted = fix_trojan_plus_issues()
        elif current_error_signature.startswith("patch_failed"):
            fix_attempted = fix_patch_application(log_content_global)
        elif current_error_signature.startswith("makefile_separator"):
            fix_attempted = fix_makefile_separator(log_content_global)
        elif current_error_signature == "directory_conflict":
            fix_attempted = fix_directory_conflict(log_content_global)
        elif current_error_signature == "symlink_conflict": # Your specific error
            fix_attempted = fix_symbolic_link_conflict(log_content_global)
        elif current_error_signature == "toolchain_provides_syntax":
            fix_attempted = fix_toolchain_provides_syntax(log_content_global)
        elif current_error_signature == "luci_lib_taskd_depends":
             fix_attempted = fix_apk_depends_problem() # Use the consolidated function
        elif current_error_signature == "apk_add_base_files":
            fix_attempted = fix_apk_add_base_files_issue(log_content_global) # Sets flag for next loop
        elif current_error_signature.startswith("makefile_dep_missing"):
            fix_attempted = fix_depends_format(log_content_global)
        elif current_error_signature.startswith("apk_add_invalid_dep_format"):
             fix_attempted = fix_apk_depends_problem() # Use the consolidated function
        elif current_error_signature == "apk_wrapper_syntax":
             fix_attempted = fix_apk_wrapper_syntax()
        elif current_error_signature == "unknown_error":
            print("æœªçŸ¥é”™è¯¯ï¼Œæ— æ³•è‡ªåŠ¨ä¿®å¤ã€‚")
            # Optional: Reduce jobs as a last resort?
            # if jobs > 1:
            #     jobs = max(1, jobs // 2)
            #     print(f"å°è¯•å‡å°‘ jobs åˆ° {jobs} ä½œä¸ºåå¤‡æªæ–½")
            #     fix_attempted = True
        elif current_error_signature in ["no_log_content", "no_log_content_error", "log_read_error"]:
             print("æ— æ³•è¯»å–æ—¥å¿—æˆ–æ— å†…å®¹ï¼Œæ— æ³•åˆ†æé”™è¯¯ã€‚")
        else:
             print(f"æœªå¤„ç†çš„é”™è¯¯ç±»å‹: {current_error_signature}ï¼Œæ— è‡ªåŠ¨ä¿®å¤ç¨‹åºã€‚")

        # --- Prepare for next retry ---
        retry += 1
        if fix_attempted or needs_base_files_precompute:
            print("å·²å°è¯•ä¿®å¤æˆ–å°†æ‰§è¡Œé¢„å¤„ç†ï¼Œç­‰å¾… 5 ç§’...")
            time.sleep(5)
        else:
            print("æœªå°è¯•ä¿®å¤ï¼Œç­‰å¾… 2 ç§’...")
            time.sleep(2)


    # --- End of Loop ---
    print(f"\n--- ç¼–è¯‘æœ€ç»ˆå¤±è´¥ ---")
    print(f"å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({args.max_retry}) æˆ–å› è¿ç»­ç›¸åŒé”™è¯¯åœæ­¢ã€‚")
    print(f"æœ€åä¸€æ¬¡è¿è¡Œæ—¥å¿—: {current_run_log}")
    print(f"æœ€åæ£€æµ‹åˆ°çš„é”™è¯¯: {last_error_signature}")
    return 1
